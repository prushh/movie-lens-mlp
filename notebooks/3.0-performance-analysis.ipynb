{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Read results datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook all the results obtained will be reported, with particular reference to the best configuration for each model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os.path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.models.config import param_layers, param_grid_mlp, param_layers_batch, param_grid_mlp_batch\n",
    "from src.utils.const import MODEL_RESULTS_CSV, NETWORK_RESULT_CSV\n",
    "from typing import Tuple"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Useful path to data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "RESULTS_FOLDER = os.path.join('..', MODEL_RESULTS_CSV)\n",
    "MLP_RESULTS_FOLDER = os.path.join('..', NETWORK_RESULT_CSV)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read output csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "mlp_all = pd.read_csv(os.path.join(MLP_RESULTS_FOLDER, 'out_mlp_all.csv'))\n",
    "mlp_batch = pd.read_csv(os.path.join(MLP_RESULTS_FOLDER, 'out_mlp_batch.csv'))\n",
    "svm_res = pd.read_csv(os.path.join(RESULTS_FOLDER, 'out_svm.csv'))\n",
    "naive_res = pd.read_csv(os.path.join(RESULTS_FOLDER, 'best_out_naive_bayes.csv'))\n",
    "tree_res = pd.read_csv(os.path.join(RESULTS_FOLDER, 'best_out_tree_based.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find best configuration of MLP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utility function to explore results DataFrame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to find configuration with the best f1-score for each fold."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def find_max_f1_cfg(df: pd.DataFrame) -> List:\n",
    "    cfg = []\n",
    "    for fold in df['fold'].unique():\n",
    "        idx = df[df['fold'] == fold]['f1_test'].idxmax()\n",
    "        cfg.append(df.iloc[idx]['cfg'])\n",
    "    cfgs = np.unique(np.array(cfg))\n",
    "    return cfgs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration ID: [ 3. 17. 18. 35.]\n"
     ]
    }
   ],
   "source": [
    "best_cfg = find_max_f1_cfg(mlp_all)\n",
    "print(f'Best configuration ID: {best_cfg}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having performed cross validation with several test sets, it is possible to obtain the mean value of the specified metric and its confidence interval, with 90% accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def mu_confidence_interval(data: np.ndarray) -> {}:\n",
    "    t = 2.13\n",
    "    mu = np.mean(data)\n",
    "    standard_deviation = np.std(data)\n",
    "    M = data.shape[0]\n",
    "    t_student = t * standard_deviation / np.sqrt(M)\n",
    "    first_interval = mu - t_student\n",
    "    second_interval = mu + t_student\n",
    "    return {\n",
    "        'mu': mu,\n",
    "        't_student': t_student,\n",
    "        'first_interval': first_interval,\n",
    "        'second_interval': second_interval\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to find the best configuration between the indexes that we found previously, this function calculate the mean on each configuration between the folds, and select the one which has the higher mean."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def find_best_conf(lst_conf, df: pd.DataFrame) -> dict:\n",
    "    conf = []\n",
    "    for idx, cfg in enumerate(lst_conf):\n",
    "        conf.append(\n",
    "            {\n",
    "                'f1': mu_confidence_interval(df[df['cfg'] == cfg]['f1_test']),\n",
    "                'loss': mu_confidence_interval(df[df['cfg'] == cfg]['f1_test']),\n",
    "                'acc': mu_confidence_interval(df[df['cfg'] == cfg]['f1_test'])\n",
    "            }\n",
    "        )\n",
    "        conf[idx]['conf'] = cfg\n",
    "    max = conf[0]\n",
    "\n",
    "    for elm in conf:\n",
    "        if max['f1']['mu'] < elm['f1']['mu']:\n",
    "            max = elm\n",
    "    return max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since in the output files containing the results of the neural network, the configuration is stored via the index, it is necessary to recalculate all configurations and select only the one of interest, specifying the index."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_best_configuration_mlp(cfg: int, p_layer, p_grid_mlp) -> Tuple:\n",
    "    hyper_parameters_model_all = itertools.product(\n",
    "        p_layer['input_act'],\n",
    "        p_layer['hidden_act'],\n",
    "        p_layer['hidden_size'],\n",
    "        p_layer['num_hidden_layers'],\n",
    "        p_layer['dropout'],\n",
    "        p_layer['batch_norm'],\n",
    "        p_layer['output_fn'],\n",
    "        p_grid_mlp['num_epochs'],\n",
    "        p_grid_mlp['starting_lr'],\n",
    "        p_grid_mlp['batch_size'],\n",
    "        p_grid_mlp['optim'],\n",
    "        p_grid_mlp['momentum'],\n",
    "        p_grid_mlp['weight_decay'],\n",
    "    )\n",
    "    return list(hyper_parameters_model_all)[cfg]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utility function to summary the calculated statistics and the relative configuration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df_test_metric = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def summary_statistics_model(df_score: pd.DataFrame, dictionary: Dict, model: str) -> pd.DataFrame:\n",
    "    print(\n",
    "        f\"Best configuration {model} mean metrics:\\n\"\n",
    "        f\"f1_score: {dictionary['f1']['mu']} ±{dictionary['f1']['t_student']}\\n\"\n",
    "        f\"loss: {dictionary['loss']['mu']} ±{dictionary['loss']['t_student']}\\n\"\n",
    "        f\"acc: {dictionary['acc']['mu']} ±{dictionary['acc']['t_student']}\\n\\n\"\n",
    "        f\"Best hyperparams configuration:\"\n",
    "    )\n",
    "    if model == \"mlp\":\n",
    "        best_cfg_mlp_all = get_best_configuration_mlp(int(dictionary['conf']), param_layers, param_grid_mlp)\n",
    "        for idx, key in enumerate(param_layers.keys()):\n",
    "            print(f\"{key}: {best_cfg_mlp_all[idx]}\")\n",
    "        for idx, key in enumerate(param_grid_mlp.keys(), 7):\n",
    "            print(f\"{key}: {best_cfg_mlp_all[idx]}\")\n",
    "    else:\n",
    "        print(f\"{dictionary['conf']}\")\n",
    "\n",
    "    new_test_score = pd.DataFrame({\n",
    "        'model': [model],\n",
    "        'f1_mu': [dictionary['f1']['mu']],\n",
    "        'acc_mu': [dictionary['acc']['mu']],\n",
    "        'loss_mu': [dictionary['loss']['mu']],\n",
    "        'f1_ci': [dictionary['f1']['t_student']],\n",
    "        'acc_ci': [dictionary['acc']['t_student']],\n",
    "        'loss_ci': [dictionary['loss']['t_student']],\n",
    "    })\n",
    "    df_score = pd.concat([df_score, new_test_score], ignore_index=True)\n",
    "\n",
    "    return df_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results best cfg mlp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration mlp mean metrics:\n",
      "f1_score: 0.8610175579840659 ±0.00978116142020269\n",
      "loss: 0.8610175579840659 ±0.00978116142020269\n",
      "acc: 0.8610175579840659 ±0.00978116142020269\n",
      "\n",
      "Best hyperparams configuration:\n",
      "input_act: LeakyReLU(negative_slope=0.01)\n",
      "hidden_act: LeakyReLU(negative_slope=0.01)\n",
      "hidden_size: 512\n",
      "num_hidden_layers: 3\n",
      "dropout: 0.2\n",
      "batch_norm: True\n",
      "output_fn: None\n",
      "num_epochs: 200\n",
      "starting_lr: 0.001\n",
      "batch_size: 128\n",
      "optim: <class 'torch.optim.adam.Adam'>\n",
      "momentum: 0.9\n",
      "weight_decay: 1e-05\n"
     ]
    }
   ],
   "source": [
    "res_mlp = find_best_conf(best_cfg, mlp_all)\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_mlp, \"mlp\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### mlp with different batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "    Unnamed: 0  cfg  fold  loss_test   acc_test   f1_test  mean_loss  \\\n0            0    0     1  18.940403  13.612167  0.148006   0.289398   \n1            1    1     1  13.517536  22.813688  0.216087   0.296447   \n2            2    2     1  18.105448  12.433460  0.142572   0.301010   \n3            3    3     1   0.751302  67.984791  0.684068   0.410880   \n4            4    4     1   1.168189  49.581749  0.505397   0.611775   \n5            5    5     1   2.254341  18.631179  0.154145   0.886728   \n6            6    6     1  12.516399  19.201521  0.186588   0.806154   \n7            7    7     1   9.610622  20.152091  0.193252   0.750074   \n8            8    8     1   8.661695  25.057034  0.219253   0.712597   \n9            9    9     1   2.770374  25.893536  0.240637   0.730237   \n10          10   10     1   2.178082  28.022814  0.224635   0.788966   \n11          11   11     1   2.289579  19.125475  0.148035   0.912474   \n12          12    0     2   5.828178  22.509506  0.191131   0.440478   \n13          13    1     2   5.978181  19.695817  0.150226   0.440467   \n14          14    2     2   3.269951  30.684411  0.281673   0.463411   \n15          15    3     2   0.945817  58.821293  0.588730   0.576282   \n16          16    4     2   1.097103  53.117871  0.538862   0.734099   \n17          17    5     2   2.280688  19.809886  0.171600   0.987963   \n18          18    6     2   4.381033  23.840304  0.177178   0.911900   \n19          19    7     2   5.283458  22.547529  0.157092   0.856075   \n20          20    8     2   3.831899  32.091255  0.258347   0.813936   \n21          21    9     2   3.871399  23.536122  0.176768   0.819556   \n22          22   10     2   1.055093  56.425856  0.571810   0.867215   \n23          23   11     2   2.296078  16.197719  0.164221   0.982693   \n24          24    0     3   0.498692  79.193610  0.791973   0.402095   \n25          25    1     3   7.262911  22.936478  0.176534   0.402557   \n26          26    2     3   0.579616  74.971472  0.750021   0.420268   \n27          27    3     3   0.780494  65.994675  0.662866   0.528486   \n28          28    4     3   2.030025  28.527957  0.255888   0.687300   \n29          29    5     3   2.291271  12.248003  0.107747   0.950278   \n30          30    6     3   5.458804  22.898440  0.178341   0.871154   \n31          31    7     3   3.692444  32.027387  0.277362   0.816153   \n32          32    8     3   4.457298  27.577025  0.209407   0.778215   \n33          33    9     3   2.464701  30.315709  0.264350   0.790917   \n34          34   10     3   1.924381  30.962343  0.267599   0.855967   \n35          35   11     3   2.296966  18.372005  0.117269   0.974238   \n36          36    0     4   7.403112  16.964625  0.119128   0.454879   \n37          37    1     4   4.995937  24.724230  0.219541   0.449225   \n38          38    2     4   0.658782  72.917459  0.728823   0.462066   \n39          39    3     4   3.914467  19.969570  0.155608   0.572289   \n40          40    4     4   1.882785  27.615063  0.224291   0.756923   \n41          41    5     4   2.257028  15.252948  0.135088   1.007771   \n42          42    6     4   7.386781  17.991632  0.150818   0.921627   \n43          43    7     4   6.499929  16.774439  0.146436   0.861984   \n44          44    8     4   0.564435  75.275770  0.752554   0.818962   \n45          45    9     4   4.230159  24.800304  0.194789   0.816713   \n46          46   10     4   2.133507  32.293648  0.255840   0.869075   \n47          47   11     4   2.289704  22.365919  0.122689   0.984491   \n48          48    0     5   7.434802  24.381894  0.176671   0.379040   \n49          49    1     5   0.514022  78.965386  0.789361   0.383768   \n50          50    2     5   7.100224  13.427159  0.097930   0.404577   \n51          51    3     5   3.982442  17.839483  0.140550   0.497902   \n52          52    4     5   3.074195  17.230886  0.109658   0.667668   \n53          53    5     5   2.298211   5.325219  0.038804   0.935022   \n54          54    6     5   4.851910  21.681248  0.157890   0.859018   \n55          55    7     5   6.053369  12.818562  0.105500   0.806396   \n56          56    8     5   7.487917  11.639407  0.079087   0.769866   \n57          57    9     5   2.688849  29.478889  0.252266   0.787365   \n58          58   10     5   1.193308  49.258273  0.504290   0.848962   \n59          59   11     5   2.291784  27.729175  0.160606   0.967607   \n\n    std_loss  mean_acc_val  std_acc_val  mean_acc_train  std_acc_train  \\\n0   0.013900     75.375842     0.475101       88.254775       0.562407   \n1   0.022876     75.671540     1.257495       87.909500       0.968884   \n2   0.020293     75.823471     1.143876       87.699934       0.865473   \n3   0.191540     72.575773     5.793029       83.027949       8.142960   \n4   0.437051     66.671148    12.934831       74.474675      18.603783   \n5   0.732925     57.278686    24.252981       65.782976      25.818213   \n6   0.706682     59.659431    23.200582       68.741819      24.977831   \n7   0.677493     61.345407    22.158502       70.762288      23.968632   \n8   0.647498     62.370921    21.092658       72.070202      22.899502   \n9   0.616622     61.641182    20.130258       71.050573      21.942100   \n10  0.616590     60.105156    19.802989       68.309591      22.646509   \n11  0.718543     56.219784    22.985344       64.405776      25.264968   \n12  0.017572     66.565125     0.532008       81.254468       0.825879   \n13  0.014978     66.798837     0.596181       81.240863       0.712141   \n14  0.036441     66.380344     1.100915       80.200721       1.653451   \n15  0.198349     63.183201     5.635736       75.401871       8.446124   \n16  0.362491     59.671546     8.654326       68.577224      15.622986   \n17  0.657070     50.940175    21.130167       60.606331      22.831350   \n18  0.636284     53.140174    20.292666       63.435018      22.248174   \n19  0.613310     54.892082    19.546129       65.507924      21.526086   \n20  0.590458     56.451260    18.963056       67.051285      20.762889   \n21  0.560516     56.974183    18.059832       66.650866      19.739054   \n22  0.555382     56.200553    17.400038       64.408098      20.115798   \n23  0.655311     52.369068    20.976534       60.876323      22.543751   \n24  0.020770     71.527192     0.844426       82.760444       0.924812   \n25  0.015134     71.453508     0.706731       82.724424       0.688173   \n26  0.028524     70.831526     1.255325       81.968118       1.241464   \n27  0.189604     68.102729     4.863261       77.220625       8.315313   \n28  0.360728     63.893221     9.516253       70.354955      15.642839   \n29  0.673963     55.585708    20.673400       62.361073      22.888275   \n30  0.653417     57.757004    19.867764       65.323740      22.401107   \n31  0.628325     59.153993    18.955033       67.345984      21.628257   \n32  0.602080     60.107457    18.077431       68.718008      20.760638   \n33  0.572708     59.624082    17.219157       67.984134      19.829306   \n34  0.583564     57.694033    17.522440       65.053532      21.057903   \n35  0.682670     53.926683    20.937119       61.466154      23.418681   \n36  0.023495     67.117157     2.180876       80.671241       1.000569   \n37  0.023501     67.725341     1.850006       80.889580       1.020508   \n38  0.027778     67.479633     1.639547       80.359702       1.166481   \n39  0.194547     64.378957     5.626853       75.627289       8.348169   \n40  0.409478     59.305896    11.341373       67.597410      17.773932   \n41  0.674059     50.742770    21.892341       59.901167      23.655655   \n42  0.658823     53.315866    21.228828       63.148800      23.304230   \n43  0.636203     55.231389    20.495852       65.346234      22.563879   \n44  0.612088     56.703336    19.776725       66.907616      21.730683   \n45  0.580806     57.408929    18.887688       66.807388      20.621745   \n46  0.578446     56.383943    18.330126       64.336658      21.176163   \n47  0.673236     52.656545    21.565106       60.811820      23.408145   \n48  0.012830     72.055484     0.784554       83.932587       0.649882   \n49  0.013711     72.149370     0.810250       83.764348       0.655411   \n50  0.033809     72.014375     0.990010       82.838593       1.522677   \n51  0.164971     70.135079     3.399416       78.781306       7.195614   \n52  0.370346     65.889755     9.035906       71.387758      16.131524   \n53  0.686798     57.961551    20.015809       62.994077      23.858967   \n54  0.662606     59.723316    19.031802       65.827806      23.156954   \n55  0.635335     61.026003    18.140484       67.747236      22.253673   \n56  0.607868     61.886208    17.275525       69.050499      21.303726   \n57  0.579229     61.261665    16.501380       68.035006      20.456959   \n58  0.585684     59.482351    16.710802       65.248626      21.403909   \n59  0.685043     55.896389    19.993290       61.596576      23.805249   \n\n    mean_f1_train  std_f1_train  mean_f1_val  std_f1_val  \n0        0.881785      0.005654     0.754430    0.003962  \n1        0.878310      0.009760     0.757455    0.012299  \n2        0.876271      0.008692     0.758931    0.011460  \n3        0.828278      0.083659     0.727154    0.056903  \n4        0.739389      0.193005     0.669061    0.127240  \n5        0.640302      0.283215     0.570275    0.250451  \n6        0.672284      0.273661     0.594345    0.239270  \n7        0.694280      0.262520     0.611319    0.228306  \n8        0.708747      0.250875     0.621693    0.217251  \n9        0.699167      0.239762     0.614613    0.207203  \n10       0.671101      0.245237     0.599881    0.203026  \n11       0.627610      0.275711     0.557799    0.239801  \n12       0.811308      0.008362     0.664787    0.005823  \n13       0.811321      0.007279     0.666611    0.007171  \n14       0.800896      0.016670     0.662219    0.011439  \n15       0.751580      0.086754     0.630598    0.055812  \n16       0.679587      0.163761     0.595991    0.085422  \n17       0.586325      0.256690     0.504586    0.219380  \n18       0.617256      0.249467     0.527081    0.210461  \n19       0.639975      0.241013     0.545022    0.202570  \n20       0.657006      0.232313     0.560969    0.196379  \n21       0.653655      0.220674     0.566869    0.187156  \n22       0.630522      0.222801     0.559650    0.179962  \n23       0.588664      0.254583     0.519160    0.218694  \n24       0.826631      0.009277     0.714694    0.008890  \n25       0.826210      0.006931     0.713868    0.007480  \n26       0.818665      0.012422     0.707843    0.012323  \n27       0.770146      0.084950     0.680688    0.048409  \n28       0.697595      0.164113     0.638587    0.095062  \n29       0.604692      0.256348     0.546041    0.225549  \n30       0.636881      0.250110     0.569011    0.216289  \n31       0.659031      0.241200     0.583903    0.206184  \n32       0.674229      0.231463     0.594184    0.196587  \n33       0.667585      0.220603     0.590174    0.186964  \n34       0.637320      0.231119     0.571981    0.187379  \n35       0.596121      0.260191     0.531046    0.225240  \n36       0.805536      0.010144     0.668982    0.021197  \n37       0.807827      0.010274     0.675478    0.018249  \n38       0.802527      0.011716     0.673000    0.016211  \n39       0.753948      0.085678     0.642936    0.054567  \n40       0.669750      0.185568     0.593853    0.109781  \n41       0.578482      0.265252     0.502270    0.228655  \n42       0.613764      0.260370     0.528564    0.221316  \n43       0.637829      0.251763     0.548161    0.213433  \n44       0.655054      0.242347     0.563376    0.205875  \n45       0.654885      0.229950     0.571124    0.196760  \n46       0.629266      0.233965     0.561356    0.190386  \n47       0.588282      0.262187     0.522276    0.224432  \n48       0.838400      0.006527     0.720647    0.007492  \n49       0.836791      0.006557     0.721526    0.008012  \n50       0.827552      0.015227     0.720188    0.009959  \n51       0.786153      0.073349     0.701487    0.033779  \n52       0.708343      0.168921     0.659317    0.089739  \n53       0.612546      0.264160     0.570088    0.217341  \n54       0.643240      0.255887     0.588873    0.206459  \n55       0.664217      0.245756     0.602901    0.196729  \n56       0.678644      0.235279     0.612221    0.187346  \n57       0.669173      0.225175     0.606719    0.178565  \n58       0.640269      0.233360     0.590158    0.178144  \n59       0.598251      0.263349     0.550737    0.215149  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>cfg</th>\n      <th>fold</th>\n      <th>loss_test</th>\n      <th>acc_test</th>\n      <th>f1_test</th>\n      <th>mean_loss</th>\n      <th>std_loss</th>\n      <th>mean_acc_val</th>\n      <th>std_acc_val</th>\n      <th>mean_acc_train</th>\n      <th>std_acc_train</th>\n      <th>mean_f1_train</th>\n      <th>std_f1_train</th>\n      <th>mean_f1_val</th>\n      <th>std_f1_val</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>18.940403</td>\n      <td>13.612167</td>\n      <td>0.148006</td>\n      <td>0.289398</td>\n      <td>0.013900</td>\n      <td>75.375842</td>\n      <td>0.475101</td>\n      <td>88.254775</td>\n      <td>0.562407</td>\n      <td>0.881785</td>\n      <td>0.005654</td>\n      <td>0.754430</td>\n      <td>0.003962</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13.517536</td>\n      <td>22.813688</td>\n      <td>0.216087</td>\n      <td>0.296447</td>\n      <td>0.022876</td>\n      <td>75.671540</td>\n      <td>1.257495</td>\n      <td>87.909500</td>\n      <td>0.968884</td>\n      <td>0.878310</td>\n      <td>0.009760</td>\n      <td>0.757455</td>\n      <td>0.012299</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>18.105448</td>\n      <td>12.433460</td>\n      <td>0.142572</td>\n      <td>0.301010</td>\n      <td>0.020293</td>\n      <td>75.823471</td>\n      <td>1.143876</td>\n      <td>87.699934</td>\n      <td>0.865473</td>\n      <td>0.876271</td>\n      <td>0.008692</td>\n      <td>0.758931</td>\n      <td>0.011460</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.751302</td>\n      <td>67.984791</td>\n      <td>0.684068</td>\n      <td>0.410880</td>\n      <td>0.191540</td>\n      <td>72.575773</td>\n      <td>5.793029</td>\n      <td>83.027949</td>\n      <td>8.142960</td>\n      <td>0.828278</td>\n      <td>0.083659</td>\n      <td>0.727154</td>\n      <td>0.056903</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1.168189</td>\n      <td>49.581749</td>\n      <td>0.505397</td>\n      <td>0.611775</td>\n      <td>0.437051</td>\n      <td>66.671148</td>\n      <td>12.934831</td>\n      <td>74.474675</td>\n      <td>18.603783</td>\n      <td>0.739389</td>\n      <td>0.193005</td>\n      <td>0.669061</td>\n      <td>0.127240</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2.254341</td>\n      <td>18.631179</td>\n      <td>0.154145</td>\n      <td>0.886728</td>\n      <td>0.732925</td>\n      <td>57.278686</td>\n      <td>24.252981</td>\n      <td>65.782976</td>\n      <td>25.818213</td>\n      <td>0.640302</td>\n      <td>0.283215</td>\n      <td>0.570275</td>\n      <td>0.250451</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>6</td>\n      <td>1</td>\n      <td>12.516399</td>\n      <td>19.201521</td>\n      <td>0.186588</td>\n      <td>0.806154</td>\n      <td>0.706682</td>\n      <td>59.659431</td>\n      <td>23.200582</td>\n      <td>68.741819</td>\n      <td>24.977831</td>\n      <td>0.672284</td>\n      <td>0.273661</td>\n      <td>0.594345</td>\n      <td>0.239270</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>7</td>\n      <td>1</td>\n      <td>9.610622</td>\n      <td>20.152091</td>\n      <td>0.193252</td>\n      <td>0.750074</td>\n      <td>0.677493</td>\n      <td>61.345407</td>\n      <td>22.158502</td>\n      <td>70.762288</td>\n      <td>23.968632</td>\n      <td>0.694280</td>\n      <td>0.262520</td>\n      <td>0.611319</td>\n      <td>0.228306</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>8</td>\n      <td>1</td>\n      <td>8.661695</td>\n      <td>25.057034</td>\n      <td>0.219253</td>\n      <td>0.712597</td>\n      <td>0.647498</td>\n      <td>62.370921</td>\n      <td>21.092658</td>\n      <td>72.070202</td>\n      <td>22.899502</td>\n      <td>0.708747</td>\n      <td>0.250875</td>\n      <td>0.621693</td>\n      <td>0.217251</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>9</td>\n      <td>1</td>\n      <td>2.770374</td>\n      <td>25.893536</td>\n      <td>0.240637</td>\n      <td>0.730237</td>\n      <td>0.616622</td>\n      <td>61.641182</td>\n      <td>20.130258</td>\n      <td>71.050573</td>\n      <td>21.942100</td>\n      <td>0.699167</td>\n      <td>0.239762</td>\n      <td>0.614613</td>\n      <td>0.207203</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>10</td>\n      <td>1</td>\n      <td>2.178082</td>\n      <td>28.022814</td>\n      <td>0.224635</td>\n      <td>0.788966</td>\n      <td>0.616590</td>\n      <td>60.105156</td>\n      <td>19.802989</td>\n      <td>68.309591</td>\n      <td>22.646509</td>\n      <td>0.671101</td>\n      <td>0.245237</td>\n      <td>0.599881</td>\n      <td>0.203026</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>11</td>\n      <td>1</td>\n      <td>2.289579</td>\n      <td>19.125475</td>\n      <td>0.148035</td>\n      <td>0.912474</td>\n      <td>0.718543</td>\n      <td>56.219784</td>\n      <td>22.985344</td>\n      <td>64.405776</td>\n      <td>25.264968</td>\n      <td>0.627610</td>\n      <td>0.275711</td>\n      <td>0.557799</td>\n      <td>0.239801</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>0</td>\n      <td>2</td>\n      <td>5.828178</td>\n      <td>22.509506</td>\n      <td>0.191131</td>\n      <td>0.440478</td>\n      <td>0.017572</td>\n      <td>66.565125</td>\n      <td>0.532008</td>\n      <td>81.254468</td>\n      <td>0.825879</td>\n      <td>0.811308</td>\n      <td>0.008362</td>\n      <td>0.664787</td>\n      <td>0.005823</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5.978181</td>\n      <td>19.695817</td>\n      <td>0.150226</td>\n      <td>0.440467</td>\n      <td>0.014978</td>\n      <td>66.798837</td>\n      <td>0.596181</td>\n      <td>81.240863</td>\n      <td>0.712141</td>\n      <td>0.811321</td>\n      <td>0.007279</td>\n      <td>0.666611</td>\n      <td>0.007171</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3.269951</td>\n      <td>30.684411</td>\n      <td>0.281673</td>\n      <td>0.463411</td>\n      <td>0.036441</td>\n      <td>66.380344</td>\n      <td>1.100915</td>\n      <td>80.200721</td>\n      <td>1.653451</td>\n      <td>0.800896</td>\n      <td>0.016670</td>\n      <td>0.662219</td>\n      <td>0.011439</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0.945817</td>\n      <td>58.821293</td>\n      <td>0.588730</td>\n      <td>0.576282</td>\n      <td>0.198349</td>\n      <td>63.183201</td>\n      <td>5.635736</td>\n      <td>75.401871</td>\n      <td>8.446124</td>\n      <td>0.751580</td>\n      <td>0.086754</td>\n      <td>0.630598</td>\n      <td>0.055812</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1.097103</td>\n      <td>53.117871</td>\n      <td>0.538862</td>\n      <td>0.734099</td>\n      <td>0.362491</td>\n      <td>59.671546</td>\n      <td>8.654326</td>\n      <td>68.577224</td>\n      <td>15.622986</td>\n      <td>0.679587</td>\n      <td>0.163761</td>\n      <td>0.595991</td>\n      <td>0.085422</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2.280688</td>\n      <td>19.809886</td>\n      <td>0.171600</td>\n      <td>0.987963</td>\n      <td>0.657070</td>\n      <td>50.940175</td>\n      <td>21.130167</td>\n      <td>60.606331</td>\n      <td>22.831350</td>\n      <td>0.586325</td>\n      <td>0.256690</td>\n      <td>0.504586</td>\n      <td>0.219380</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>6</td>\n      <td>2</td>\n      <td>4.381033</td>\n      <td>23.840304</td>\n      <td>0.177178</td>\n      <td>0.911900</td>\n      <td>0.636284</td>\n      <td>53.140174</td>\n      <td>20.292666</td>\n      <td>63.435018</td>\n      <td>22.248174</td>\n      <td>0.617256</td>\n      <td>0.249467</td>\n      <td>0.527081</td>\n      <td>0.210461</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>7</td>\n      <td>2</td>\n      <td>5.283458</td>\n      <td>22.547529</td>\n      <td>0.157092</td>\n      <td>0.856075</td>\n      <td>0.613310</td>\n      <td>54.892082</td>\n      <td>19.546129</td>\n      <td>65.507924</td>\n      <td>21.526086</td>\n      <td>0.639975</td>\n      <td>0.241013</td>\n      <td>0.545022</td>\n      <td>0.202570</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>8</td>\n      <td>2</td>\n      <td>3.831899</td>\n      <td>32.091255</td>\n      <td>0.258347</td>\n      <td>0.813936</td>\n      <td>0.590458</td>\n      <td>56.451260</td>\n      <td>18.963056</td>\n      <td>67.051285</td>\n      <td>20.762889</td>\n      <td>0.657006</td>\n      <td>0.232313</td>\n      <td>0.560969</td>\n      <td>0.196379</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>9</td>\n      <td>2</td>\n      <td>3.871399</td>\n      <td>23.536122</td>\n      <td>0.176768</td>\n      <td>0.819556</td>\n      <td>0.560516</td>\n      <td>56.974183</td>\n      <td>18.059832</td>\n      <td>66.650866</td>\n      <td>19.739054</td>\n      <td>0.653655</td>\n      <td>0.220674</td>\n      <td>0.566869</td>\n      <td>0.187156</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>10</td>\n      <td>2</td>\n      <td>1.055093</td>\n      <td>56.425856</td>\n      <td>0.571810</td>\n      <td>0.867215</td>\n      <td>0.555382</td>\n      <td>56.200553</td>\n      <td>17.400038</td>\n      <td>64.408098</td>\n      <td>20.115798</td>\n      <td>0.630522</td>\n      <td>0.222801</td>\n      <td>0.559650</td>\n      <td>0.179962</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>11</td>\n      <td>2</td>\n      <td>2.296078</td>\n      <td>16.197719</td>\n      <td>0.164221</td>\n      <td>0.982693</td>\n      <td>0.655311</td>\n      <td>52.369068</td>\n      <td>20.976534</td>\n      <td>60.876323</td>\n      <td>22.543751</td>\n      <td>0.588664</td>\n      <td>0.254583</td>\n      <td>0.519160</td>\n      <td>0.218694</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.498692</td>\n      <td>79.193610</td>\n      <td>0.791973</td>\n      <td>0.402095</td>\n      <td>0.020770</td>\n      <td>71.527192</td>\n      <td>0.844426</td>\n      <td>82.760444</td>\n      <td>0.924812</td>\n      <td>0.826631</td>\n      <td>0.009277</td>\n      <td>0.714694</td>\n      <td>0.008890</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>1</td>\n      <td>3</td>\n      <td>7.262911</td>\n      <td>22.936478</td>\n      <td>0.176534</td>\n      <td>0.402557</td>\n      <td>0.015134</td>\n      <td>71.453508</td>\n      <td>0.706731</td>\n      <td>82.724424</td>\n      <td>0.688173</td>\n      <td>0.826210</td>\n      <td>0.006931</td>\n      <td>0.713868</td>\n      <td>0.007480</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.579616</td>\n      <td>74.971472</td>\n      <td>0.750021</td>\n      <td>0.420268</td>\n      <td>0.028524</td>\n      <td>70.831526</td>\n      <td>1.255325</td>\n      <td>81.968118</td>\n      <td>1.241464</td>\n      <td>0.818665</td>\n      <td>0.012422</td>\n      <td>0.707843</td>\n      <td>0.012323</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.780494</td>\n      <td>65.994675</td>\n      <td>0.662866</td>\n      <td>0.528486</td>\n      <td>0.189604</td>\n      <td>68.102729</td>\n      <td>4.863261</td>\n      <td>77.220625</td>\n      <td>8.315313</td>\n      <td>0.770146</td>\n      <td>0.084950</td>\n      <td>0.680688</td>\n      <td>0.048409</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2.030025</td>\n      <td>28.527957</td>\n      <td>0.255888</td>\n      <td>0.687300</td>\n      <td>0.360728</td>\n      <td>63.893221</td>\n      <td>9.516253</td>\n      <td>70.354955</td>\n      <td>15.642839</td>\n      <td>0.697595</td>\n      <td>0.164113</td>\n      <td>0.638587</td>\n      <td>0.095062</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>5</td>\n      <td>3</td>\n      <td>2.291271</td>\n      <td>12.248003</td>\n      <td>0.107747</td>\n      <td>0.950278</td>\n      <td>0.673963</td>\n      <td>55.585708</td>\n      <td>20.673400</td>\n      <td>62.361073</td>\n      <td>22.888275</td>\n      <td>0.604692</td>\n      <td>0.256348</td>\n      <td>0.546041</td>\n      <td>0.225549</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>30</td>\n      <td>6</td>\n      <td>3</td>\n      <td>5.458804</td>\n      <td>22.898440</td>\n      <td>0.178341</td>\n      <td>0.871154</td>\n      <td>0.653417</td>\n      <td>57.757004</td>\n      <td>19.867764</td>\n      <td>65.323740</td>\n      <td>22.401107</td>\n      <td>0.636881</td>\n      <td>0.250110</td>\n      <td>0.569011</td>\n      <td>0.216289</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>31</td>\n      <td>7</td>\n      <td>3</td>\n      <td>3.692444</td>\n      <td>32.027387</td>\n      <td>0.277362</td>\n      <td>0.816153</td>\n      <td>0.628325</td>\n      <td>59.153993</td>\n      <td>18.955033</td>\n      <td>67.345984</td>\n      <td>21.628257</td>\n      <td>0.659031</td>\n      <td>0.241200</td>\n      <td>0.583903</td>\n      <td>0.206184</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>8</td>\n      <td>3</td>\n      <td>4.457298</td>\n      <td>27.577025</td>\n      <td>0.209407</td>\n      <td>0.778215</td>\n      <td>0.602080</td>\n      <td>60.107457</td>\n      <td>18.077431</td>\n      <td>68.718008</td>\n      <td>20.760638</td>\n      <td>0.674229</td>\n      <td>0.231463</td>\n      <td>0.594184</td>\n      <td>0.196587</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>33</td>\n      <td>9</td>\n      <td>3</td>\n      <td>2.464701</td>\n      <td>30.315709</td>\n      <td>0.264350</td>\n      <td>0.790917</td>\n      <td>0.572708</td>\n      <td>59.624082</td>\n      <td>17.219157</td>\n      <td>67.984134</td>\n      <td>19.829306</td>\n      <td>0.667585</td>\n      <td>0.220603</td>\n      <td>0.590174</td>\n      <td>0.186964</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>34</td>\n      <td>10</td>\n      <td>3</td>\n      <td>1.924381</td>\n      <td>30.962343</td>\n      <td>0.267599</td>\n      <td>0.855967</td>\n      <td>0.583564</td>\n      <td>57.694033</td>\n      <td>17.522440</td>\n      <td>65.053532</td>\n      <td>21.057903</td>\n      <td>0.637320</td>\n      <td>0.231119</td>\n      <td>0.571981</td>\n      <td>0.187379</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>35</td>\n      <td>11</td>\n      <td>3</td>\n      <td>2.296966</td>\n      <td>18.372005</td>\n      <td>0.117269</td>\n      <td>0.974238</td>\n      <td>0.682670</td>\n      <td>53.926683</td>\n      <td>20.937119</td>\n      <td>61.466154</td>\n      <td>23.418681</td>\n      <td>0.596121</td>\n      <td>0.260191</td>\n      <td>0.531046</td>\n      <td>0.225240</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>36</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7.403112</td>\n      <td>16.964625</td>\n      <td>0.119128</td>\n      <td>0.454879</td>\n      <td>0.023495</td>\n      <td>67.117157</td>\n      <td>2.180876</td>\n      <td>80.671241</td>\n      <td>1.000569</td>\n      <td>0.805536</td>\n      <td>0.010144</td>\n      <td>0.668982</td>\n      <td>0.021197</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>37</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4.995937</td>\n      <td>24.724230</td>\n      <td>0.219541</td>\n      <td>0.449225</td>\n      <td>0.023501</td>\n      <td>67.725341</td>\n      <td>1.850006</td>\n      <td>80.889580</td>\n      <td>1.020508</td>\n      <td>0.807827</td>\n      <td>0.010274</td>\n      <td>0.675478</td>\n      <td>0.018249</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>38</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0.658782</td>\n      <td>72.917459</td>\n      <td>0.728823</td>\n      <td>0.462066</td>\n      <td>0.027778</td>\n      <td>67.479633</td>\n      <td>1.639547</td>\n      <td>80.359702</td>\n      <td>1.166481</td>\n      <td>0.802527</td>\n      <td>0.011716</td>\n      <td>0.673000</td>\n      <td>0.016211</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>39</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3.914467</td>\n      <td>19.969570</td>\n      <td>0.155608</td>\n      <td>0.572289</td>\n      <td>0.194547</td>\n      <td>64.378957</td>\n      <td>5.626853</td>\n      <td>75.627289</td>\n      <td>8.348169</td>\n      <td>0.753948</td>\n      <td>0.085678</td>\n      <td>0.642936</td>\n      <td>0.054567</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>40</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1.882785</td>\n      <td>27.615063</td>\n      <td>0.224291</td>\n      <td>0.756923</td>\n      <td>0.409478</td>\n      <td>59.305896</td>\n      <td>11.341373</td>\n      <td>67.597410</td>\n      <td>17.773932</td>\n      <td>0.669750</td>\n      <td>0.185568</td>\n      <td>0.593853</td>\n      <td>0.109781</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>41</td>\n      <td>5</td>\n      <td>4</td>\n      <td>2.257028</td>\n      <td>15.252948</td>\n      <td>0.135088</td>\n      <td>1.007771</td>\n      <td>0.674059</td>\n      <td>50.742770</td>\n      <td>21.892341</td>\n      <td>59.901167</td>\n      <td>23.655655</td>\n      <td>0.578482</td>\n      <td>0.265252</td>\n      <td>0.502270</td>\n      <td>0.228655</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>42</td>\n      <td>6</td>\n      <td>4</td>\n      <td>7.386781</td>\n      <td>17.991632</td>\n      <td>0.150818</td>\n      <td>0.921627</td>\n      <td>0.658823</td>\n      <td>53.315866</td>\n      <td>21.228828</td>\n      <td>63.148800</td>\n      <td>23.304230</td>\n      <td>0.613764</td>\n      <td>0.260370</td>\n      <td>0.528564</td>\n      <td>0.221316</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>43</td>\n      <td>7</td>\n      <td>4</td>\n      <td>6.499929</td>\n      <td>16.774439</td>\n      <td>0.146436</td>\n      <td>0.861984</td>\n      <td>0.636203</td>\n      <td>55.231389</td>\n      <td>20.495852</td>\n      <td>65.346234</td>\n      <td>22.563879</td>\n      <td>0.637829</td>\n      <td>0.251763</td>\n      <td>0.548161</td>\n      <td>0.213433</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>44</td>\n      <td>8</td>\n      <td>4</td>\n      <td>0.564435</td>\n      <td>75.275770</td>\n      <td>0.752554</td>\n      <td>0.818962</td>\n      <td>0.612088</td>\n      <td>56.703336</td>\n      <td>19.776725</td>\n      <td>66.907616</td>\n      <td>21.730683</td>\n      <td>0.655054</td>\n      <td>0.242347</td>\n      <td>0.563376</td>\n      <td>0.205875</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>45</td>\n      <td>9</td>\n      <td>4</td>\n      <td>4.230159</td>\n      <td>24.800304</td>\n      <td>0.194789</td>\n      <td>0.816713</td>\n      <td>0.580806</td>\n      <td>57.408929</td>\n      <td>18.887688</td>\n      <td>66.807388</td>\n      <td>20.621745</td>\n      <td>0.654885</td>\n      <td>0.229950</td>\n      <td>0.571124</td>\n      <td>0.196760</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>46</td>\n      <td>10</td>\n      <td>4</td>\n      <td>2.133507</td>\n      <td>32.293648</td>\n      <td>0.255840</td>\n      <td>0.869075</td>\n      <td>0.578446</td>\n      <td>56.383943</td>\n      <td>18.330126</td>\n      <td>64.336658</td>\n      <td>21.176163</td>\n      <td>0.629266</td>\n      <td>0.233965</td>\n      <td>0.561356</td>\n      <td>0.190386</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>47</td>\n      <td>11</td>\n      <td>4</td>\n      <td>2.289704</td>\n      <td>22.365919</td>\n      <td>0.122689</td>\n      <td>0.984491</td>\n      <td>0.673236</td>\n      <td>52.656545</td>\n      <td>21.565106</td>\n      <td>60.811820</td>\n      <td>23.408145</td>\n      <td>0.588282</td>\n      <td>0.262187</td>\n      <td>0.522276</td>\n      <td>0.224432</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>48</td>\n      <td>0</td>\n      <td>5</td>\n      <td>7.434802</td>\n      <td>24.381894</td>\n      <td>0.176671</td>\n      <td>0.379040</td>\n      <td>0.012830</td>\n      <td>72.055484</td>\n      <td>0.784554</td>\n      <td>83.932587</td>\n      <td>0.649882</td>\n      <td>0.838400</td>\n      <td>0.006527</td>\n      <td>0.720647</td>\n      <td>0.007492</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>49</td>\n      <td>1</td>\n      <td>5</td>\n      <td>0.514022</td>\n      <td>78.965386</td>\n      <td>0.789361</td>\n      <td>0.383768</td>\n      <td>0.013711</td>\n      <td>72.149370</td>\n      <td>0.810250</td>\n      <td>83.764348</td>\n      <td>0.655411</td>\n      <td>0.836791</td>\n      <td>0.006557</td>\n      <td>0.721526</td>\n      <td>0.008012</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>50</td>\n      <td>2</td>\n      <td>5</td>\n      <td>7.100224</td>\n      <td>13.427159</td>\n      <td>0.097930</td>\n      <td>0.404577</td>\n      <td>0.033809</td>\n      <td>72.014375</td>\n      <td>0.990010</td>\n      <td>82.838593</td>\n      <td>1.522677</td>\n      <td>0.827552</td>\n      <td>0.015227</td>\n      <td>0.720188</td>\n      <td>0.009959</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>51</td>\n      <td>3</td>\n      <td>5</td>\n      <td>3.982442</td>\n      <td>17.839483</td>\n      <td>0.140550</td>\n      <td>0.497902</td>\n      <td>0.164971</td>\n      <td>70.135079</td>\n      <td>3.399416</td>\n      <td>78.781306</td>\n      <td>7.195614</td>\n      <td>0.786153</td>\n      <td>0.073349</td>\n      <td>0.701487</td>\n      <td>0.033779</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>52</td>\n      <td>4</td>\n      <td>5</td>\n      <td>3.074195</td>\n      <td>17.230886</td>\n      <td>0.109658</td>\n      <td>0.667668</td>\n      <td>0.370346</td>\n      <td>65.889755</td>\n      <td>9.035906</td>\n      <td>71.387758</td>\n      <td>16.131524</td>\n      <td>0.708343</td>\n      <td>0.168921</td>\n      <td>0.659317</td>\n      <td>0.089739</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>53</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2.298211</td>\n      <td>5.325219</td>\n      <td>0.038804</td>\n      <td>0.935022</td>\n      <td>0.686798</td>\n      <td>57.961551</td>\n      <td>20.015809</td>\n      <td>62.994077</td>\n      <td>23.858967</td>\n      <td>0.612546</td>\n      <td>0.264160</td>\n      <td>0.570088</td>\n      <td>0.217341</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>54</td>\n      <td>6</td>\n      <td>5</td>\n      <td>4.851910</td>\n      <td>21.681248</td>\n      <td>0.157890</td>\n      <td>0.859018</td>\n      <td>0.662606</td>\n      <td>59.723316</td>\n      <td>19.031802</td>\n      <td>65.827806</td>\n      <td>23.156954</td>\n      <td>0.643240</td>\n      <td>0.255887</td>\n      <td>0.588873</td>\n      <td>0.206459</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>55</td>\n      <td>7</td>\n      <td>5</td>\n      <td>6.053369</td>\n      <td>12.818562</td>\n      <td>0.105500</td>\n      <td>0.806396</td>\n      <td>0.635335</td>\n      <td>61.026003</td>\n      <td>18.140484</td>\n      <td>67.747236</td>\n      <td>22.253673</td>\n      <td>0.664217</td>\n      <td>0.245756</td>\n      <td>0.602901</td>\n      <td>0.196729</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>56</td>\n      <td>8</td>\n      <td>5</td>\n      <td>7.487917</td>\n      <td>11.639407</td>\n      <td>0.079087</td>\n      <td>0.769866</td>\n      <td>0.607868</td>\n      <td>61.886208</td>\n      <td>17.275525</td>\n      <td>69.050499</td>\n      <td>21.303726</td>\n      <td>0.678644</td>\n      <td>0.235279</td>\n      <td>0.612221</td>\n      <td>0.187346</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>57</td>\n      <td>9</td>\n      <td>5</td>\n      <td>2.688849</td>\n      <td>29.478889</td>\n      <td>0.252266</td>\n      <td>0.787365</td>\n      <td>0.579229</td>\n      <td>61.261665</td>\n      <td>16.501380</td>\n      <td>68.035006</td>\n      <td>20.456959</td>\n      <td>0.669173</td>\n      <td>0.225175</td>\n      <td>0.606719</td>\n      <td>0.178565</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>58</td>\n      <td>10</td>\n      <td>5</td>\n      <td>1.193308</td>\n      <td>49.258273</td>\n      <td>0.504290</td>\n      <td>0.848962</td>\n      <td>0.585684</td>\n      <td>59.482351</td>\n      <td>16.710802</td>\n      <td>65.248626</td>\n      <td>21.403909</td>\n      <td>0.640269</td>\n      <td>0.233360</td>\n      <td>0.590158</td>\n      <td>0.178144</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>59</td>\n      <td>11</td>\n      <td>5</td>\n      <td>2.291784</td>\n      <td>27.729175</td>\n      <td>0.160606</td>\n      <td>0.967607</td>\n      <td>0.685043</td>\n      <td>55.896389</td>\n      <td>19.993290</td>\n      <td>61.596576</td>\n      <td>23.805249</td>\n      <td>0.598251</td>\n      <td>0.263349</td>\n      <td>0.550737</td>\n      <td>0.215149</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index  batch_norm  batch_size   f1_mean  f1_confidence  loss_mean  \\\n",
      "0       0        True          16  0.285382       0.242427   8.021037   \n",
      "1       1        True          32  0.310350       0.229459   6.453717   \n",
      "2       2        True          64  0.400204       0.270151   5.942804   \n",
      "3       3        True         512  0.446364       0.233992   2.074904   \n",
      "4       4        True        2048  0.326819       0.159141   1.850459   \n",
      "5       5        True       16384  0.121477       0.044247   2.276308   \n",
      "6       6       False          16  0.170163       0.012856   6.918986   \n",
      "7       7       False          32  0.175928       0.055172   6.227965   \n",
      "8       8       False          64  0.303729       0.221356   5.000649   \n",
      "9       9       False         512  0.225762       0.032366   3.205096   \n",
      "10     10       False        2048  0.364835       0.136903   1.696874   \n",
      "11     11       False       16384  0.142564       0.018369   2.292822   \n",
      "\n",
      "    loss_confidence   acc_mean  acc_confidence  \n",
      "0          5.736820  31.332360       23.087817  \n",
      "1          4.000400  33.827120       21.553753  \n",
      "2          6.218700  40.886792       26.451277  \n",
      "3          1.458697  46.121962       21.376658  \n",
      "4          0.682113  35.214705       13.149677  \n",
      "5          0.016917  14.253447        4.945503  \n",
      "6          2.838452  21.122629        2.101917  \n",
      "7          1.850419  20.864002        6.164735  \n",
      "8          2.725434  34.328098       20.554320  \n",
      "9          0.673349  26.804912        2.520606  \n",
      "10         0.454730  39.392587       10.761874  \n",
      "11         0.002985  20.758058        3.818276  \n"
     ]
    }
   ],
   "source": [
    "for idx in mlp_batch['cfg'].unique():\n",
    "    config = get_best_configuration_mlp(idx, param_layers_batch, param_grid_mlp_batch)\n",
    "    batch_norm = config[5]\n",
    "    batch_size = config[9]\n",
    "    f1_dict = mu_confidence_interval(mlp_batch[mlp_batch['cfg'] == idx]['f1_test'])\n",
    "    loss_dict = mu_confidence_interval(mlp_batch[mlp_batch['cfg'] == idx]['loss_test'])\n",
    "    acc_dict = mu_confidence_interval(mlp_batch[mlp_batch['cfg'] == idx]['acc_test'])\n",
    "\n",
    "    new_sample = pd.DataFrame({\n",
    "        'index': [idx],\n",
    "        'batch_norm': [batch_norm],\n",
    "        'batch_size': [batch_size],\n",
    "        'f1_mean': [f1_dict['mu']],\n",
    "        'f1_confidence': [f1_dict['t_student']],\n",
    "        'loss_mean': [loss_dict['mu']],\n",
    "        'loss_confidence': [loss_dict['t_student']],\n",
    "        'acc_mean': [acc_dict['mu']],\n",
    "        'acc_confidence': [acc_dict['t_student']]\n",
    "\n",
    "    })\n",
    "    df = pd.concat([df, new_sample], ignore_index=True)\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "res_mlp = find_best_conf(best_cfg, mlp_all)\n",
    "# df_test_metric = summary_statistics_model(df_test_metric, res_mlp, \"mlp\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### mlp with different batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "    Unnamed: 0  cfg  fold  loss_test   acc_test   f1_test  mean_loss  \\\n0            0    0     1  18.940403  13.612167  0.148006   0.289398   \n1            1    1     1  13.517536  22.813688  0.216087   0.296447   \n2            2    2     1  18.105448  12.433460  0.142572   0.301010   \n3            3    3     1   0.751302  67.984791  0.684068   0.410880   \n4            4    4     1   1.168189  49.581749  0.505397   0.611775   \n5            5    5     1   2.254341  18.631179  0.154145   0.886728   \n6            6    6     1  12.516399  19.201521  0.186588   0.806154   \n7            7    7     1   9.610622  20.152091  0.193252   0.750074   \n8            8    8     1   8.661695  25.057034  0.219253   0.712597   \n9            9    9     1   2.770374  25.893536  0.240637   0.730237   \n10          10   10     1   2.178082  28.022814  0.224635   0.788966   \n11          11   11     1   2.289579  19.125475  0.148035   0.912474   \n12          12    0     2   5.828178  22.509506  0.191131   0.440478   \n13          13    1     2   5.978181  19.695817  0.150226   0.440467   \n14          14    2     2   3.269951  30.684411  0.281673   0.463411   \n15          15    3     2   0.945817  58.821293  0.588730   0.576282   \n16          16    4     2   1.097103  53.117871  0.538862   0.734099   \n17          17    5     2   2.280688  19.809886  0.171600   0.987963   \n18          18    6     2   4.381033  23.840304  0.177178   0.911900   \n19          19    7     2   5.283458  22.547529  0.157092   0.856075   \n20          20    8     2   3.831899  32.091255  0.258347   0.813936   \n21          21    9     2   3.871399  23.536122  0.176768   0.819556   \n22          22   10     2   1.055093  56.425856  0.571810   0.867215   \n23          23   11     2   2.296078  16.197719  0.164221   0.982693   \n24          24    0     3   0.498692  79.193610  0.791973   0.402095   \n25          25    1     3   7.262911  22.936478  0.176534   0.402557   \n26          26    2     3   0.579616  74.971472  0.750021   0.420268   \n27          27    3     3   0.780494  65.994675  0.662866   0.528486   \n28          28    4     3   2.030025  28.527957  0.255888   0.687300   \n29          29    5     3   2.291271  12.248003  0.107747   0.950278   \n30          30    6     3   5.458804  22.898440  0.178341   0.871154   \n31          31    7     3   3.692444  32.027387  0.277362   0.816153   \n32          32    8     3   4.457298  27.577025  0.209407   0.778215   \n33          33    9     3   2.464701  30.315709  0.264350   0.790917   \n34          34   10     3   1.924381  30.962343  0.267599   0.855967   \n35          35   11     3   2.296966  18.372005  0.117269   0.974238   \n36          36    0     4   7.403112  16.964625  0.119128   0.454879   \n37          37    1     4   4.995937  24.724230  0.219541   0.449225   \n38          38    2     4   0.658782  72.917459  0.728823   0.462066   \n39          39    3     4   3.914467  19.969570  0.155608   0.572289   \n40          40    4     4   1.882785  27.615063  0.224291   0.756923   \n41          41    5     4   2.257028  15.252948  0.135088   1.007771   \n42          42    6     4   7.386781  17.991632  0.150818   0.921627   \n43          43    7     4   6.499929  16.774439  0.146436   0.861984   \n44          44    8     4   0.564435  75.275770  0.752554   0.818962   \n45          45    9     4   4.230159  24.800304  0.194789   0.816713   \n46          46   10     4   2.133507  32.293648  0.255840   0.869075   \n47          47   11     4   2.289704  22.365919  0.122689   0.984491   \n48          48    0     5   7.434802  24.381894  0.176671   0.379040   \n49          49    1     5   0.514022  78.965386  0.789361   0.383768   \n50          50    2     5   7.100224  13.427159  0.097930   0.404577   \n51          51    3     5   3.982442  17.839483  0.140550   0.497902   \n52          52    4     5   3.074195  17.230886  0.109658   0.667668   \n53          53    5     5   2.298211   5.325219  0.038804   0.935022   \n54          54    6     5   4.851910  21.681248  0.157890   0.859018   \n55          55    7     5   6.053369  12.818562  0.105500   0.806396   \n56          56    8     5   7.487917  11.639407  0.079087   0.769866   \n57          57    9     5   2.688849  29.478889  0.252266   0.787365   \n58          58   10     5   1.193308  49.258273  0.504290   0.848962   \n59          59   11     5   2.291784  27.729175  0.160606   0.967607   \n\n    std_loss  mean_acc_val  std_acc_val  mean_acc_train  std_acc_train  \\\n0   0.013900     75.375842     0.475101       88.254775       0.562407   \n1   0.022876     75.671540     1.257495       87.909500       0.968884   \n2   0.020293     75.823471     1.143876       87.699934       0.865473   \n3   0.191540     72.575773     5.793029       83.027949       8.142960   \n4   0.437051     66.671148    12.934831       74.474675      18.603783   \n5   0.732925     57.278686    24.252981       65.782976      25.818213   \n6   0.706682     59.659431    23.200582       68.741819      24.977831   \n7   0.677493     61.345407    22.158502       70.762288      23.968632   \n8   0.647498     62.370921    21.092658       72.070202      22.899502   \n9   0.616622     61.641182    20.130258       71.050573      21.942100   \n10  0.616590     60.105156    19.802989       68.309591      22.646509   \n11  0.718543     56.219784    22.985344       64.405776      25.264968   \n12  0.017572     66.565125     0.532008       81.254468       0.825879   \n13  0.014978     66.798837     0.596181       81.240863       0.712141   \n14  0.036441     66.380344     1.100915       80.200721       1.653451   \n15  0.198349     63.183201     5.635736       75.401871       8.446124   \n16  0.362491     59.671546     8.654326       68.577224      15.622986   \n17  0.657070     50.940175    21.130167       60.606331      22.831350   \n18  0.636284     53.140174    20.292666       63.435018      22.248174   \n19  0.613310     54.892082    19.546129       65.507924      21.526086   \n20  0.590458     56.451260    18.963056       67.051285      20.762889   \n21  0.560516     56.974183    18.059832       66.650866      19.739054   \n22  0.555382     56.200553    17.400038       64.408098      20.115798   \n23  0.655311     52.369068    20.976534       60.876323      22.543751   \n24  0.020770     71.527192     0.844426       82.760444       0.924812   \n25  0.015134     71.453508     0.706731       82.724424       0.688173   \n26  0.028524     70.831526     1.255325       81.968118       1.241464   \n27  0.189604     68.102729     4.863261       77.220625       8.315313   \n28  0.360728     63.893221     9.516253       70.354955      15.642839   \n29  0.673963     55.585708    20.673400       62.361073      22.888275   \n30  0.653417     57.757004    19.867764       65.323740      22.401107   \n31  0.628325     59.153993    18.955033       67.345984      21.628257   \n32  0.602080     60.107457    18.077431       68.718008      20.760638   \n33  0.572708     59.624082    17.219157       67.984134      19.829306   \n34  0.583564     57.694033    17.522440       65.053532      21.057903   \n35  0.682670     53.926683    20.937119       61.466154      23.418681   \n36  0.023495     67.117157     2.180876       80.671241       1.000569   \n37  0.023501     67.725341     1.850006       80.889580       1.020508   \n38  0.027778     67.479633     1.639547       80.359702       1.166481   \n39  0.194547     64.378957     5.626853       75.627289       8.348169   \n40  0.409478     59.305896    11.341373       67.597410      17.773932   \n41  0.674059     50.742770    21.892341       59.901167      23.655655   \n42  0.658823     53.315866    21.228828       63.148800      23.304230   \n43  0.636203     55.231389    20.495852       65.346234      22.563879   \n44  0.612088     56.703336    19.776725       66.907616      21.730683   \n45  0.580806     57.408929    18.887688       66.807388      20.621745   \n46  0.578446     56.383943    18.330126       64.336658      21.176163   \n47  0.673236     52.656545    21.565106       60.811820      23.408145   \n48  0.012830     72.055484     0.784554       83.932587       0.649882   \n49  0.013711     72.149370     0.810250       83.764348       0.655411   \n50  0.033809     72.014375     0.990010       82.838593       1.522677   \n51  0.164971     70.135079     3.399416       78.781306       7.195614   \n52  0.370346     65.889755     9.035906       71.387758      16.131524   \n53  0.686798     57.961551    20.015809       62.994077      23.858967   \n54  0.662606     59.723316    19.031802       65.827806      23.156954   \n55  0.635335     61.026003    18.140484       67.747236      22.253673   \n56  0.607868     61.886208    17.275525       69.050499      21.303726   \n57  0.579229     61.261665    16.501380       68.035006      20.456959   \n58  0.585684     59.482351    16.710802       65.248626      21.403909   \n59  0.685043     55.896389    19.993290       61.596576      23.805249   \n\n    mean_f1_train  std_f1_train  mean_f1_val  std_f1_val  \n0        0.881785      0.005654     0.754430    0.003962  \n1        0.878310      0.009760     0.757455    0.012299  \n2        0.876271      0.008692     0.758931    0.011460  \n3        0.828278      0.083659     0.727154    0.056903  \n4        0.739389      0.193005     0.669061    0.127240  \n5        0.640302      0.283215     0.570275    0.250451  \n6        0.672284      0.273661     0.594345    0.239270  \n7        0.694280      0.262520     0.611319    0.228306  \n8        0.708747      0.250875     0.621693    0.217251  \n9        0.699167      0.239762     0.614613    0.207203  \n10       0.671101      0.245237     0.599881    0.203026  \n11       0.627610      0.275711     0.557799    0.239801  \n12       0.811308      0.008362     0.664787    0.005823  \n13       0.811321      0.007279     0.666611    0.007171  \n14       0.800896      0.016670     0.662219    0.011439  \n15       0.751580      0.086754     0.630598    0.055812  \n16       0.679587      0.163761     0.595991    0.085422  \n17       0.586325      0.256690     0.504586    0.219380  \n18       0.617256      0.249467     0.527081    0.210461  \n19       0.639975      0.241013     0.545022    0.202570  \n20       0.657006      0.232313     0.560969    0.196379  \n21       0.653655      0.220674     0.566869    0.187156  \n22       0.630522      0.222801     0.559650    0.179962  \n23       0.588664      0.254583     0.519160    0.218694  \n24       0.826631      0.009277     0.714694    0.008890  \n25       0.826210      0.006931     0.713868    0.007480  \n26       0.818665      0.012422     0.707843    0.012323  \n27       0.770146      0.084950     0.680688    0.048409  \n28       0.697595      0.164113     0.638587    0.095062  \n29       0.604692      0.256348     0.546041    0.225549  \n30       0.636881      0.250110     0.569011    0.216289  \n31       0.659031      0.241200     0.583903    0.206184  \n32       0.674229      0.231463     0.594184    0.196587  \n33       0.667585      0.220603     0.590174    0.186964  \n34       0.637320      0.231119     0.571981    0.187379  \n35       0.596121      0.260191     0.531046    0.225240  \n36       0.805536      0.010144     0.668982    0.021197  \n37       0.807827      0.010274     0.675478    0.018249  \n38       0.802527      0.011716     0.673000    0.016211  \n39       0.753948      0.085678     0.642936    0.054567  \n40       0.669750      0.185568     0.593853    0.109781  \n41       0.578482      0.265252     0.502270    0.228655  \n42       0.613764      0.260370     0.528564    0.221316  \n43       0.637829      0.251763     0.548161    0.213433  \n44       0.655054      0.242347     0.563376    0.205875  \n45       0.654885      0.229950     0.571124    0.196760  \n46       0.629266      0.233965     0.561356    0.190386  \n47       0.588282      0.262187     0.522276    0.224432  \n48       0.838400      0.006527     0.720647    0.007492  \n49       0.836791      0.006557     0.721526    0.008012  \n50       0.827552      0.015227     0.720188    0.009959  \n51       0.786153      0.073349     0.701487    0.033779  \n52       0.708343      0.168921     0.659317    0.089739  \n53       0.612546      0.264160     0.570088    0.217341  \n54       0.643240      0.255887     0.588873    0.206459  \n55       0.664217      0.245756     0.602901    0.196729  \n56       0.678644      0.235279     0.612221    0.187346  \n57       0.669173      0.225175     0.606719    0.178565  \n58       0.640269      0.233360     0.590158    0.178144  \n59       0.598251      0.263349     0.550737    0.215149  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>cfg</th>\n      <th>fold</th>\n      <th>loss_test</th>\n      <th>acc_test</th>\n      <th>f1_test</th>\n      <th>mean_loss</th>\n      <th>std_loss</th>\n      <th>mean_acc_val</th>\n      <th>std_acc_val</th>\n      <th>mean_acc_train</th>\n      <th>std_acc_train</th>\n      <th>mean_f1_train</th>\n      <th>std_f1_train</th>\n      <th>mean_f1_val</th>\n      <th>std_f1_val</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>18.940403</td>\n      <td>13.612167</td>\n      <td>0.148006</td>\n      <td>0.289398</td>\n      <td>0.013900</td>\n      <td>75.375842</td>\n      <td>0.475101</td>\n      <td>88.254775</td>\n      <td>0.562407</td>\n      <td>0.881785</td>\n      <td>0.005654</td>\n      <td>0.754430</td>\n      <td>0.003962</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13.517536</td>\n      <td>22.813688</td>\n      <td>0.216087</td>\n      <td>0.296447</td>\n      <td>0.022876</td>\n      <td>75.671540</td>\n      <td>1.257495</td>\n      <td>87.909500</td>\n      <td>0.968884</td>\n      <td>0.878310</td>\n      <td>0.009760</td>\n      <td>0.757455</td>\n      <td>0.012299</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>18.105448</td>\n      <td>12.433460</td>\n      <td>0.142572</td>\n      <td>0.301010</td>\n      <td>0.020293</td>\n      <td>75.823471</td>\n      <td>1.143876</td>\n      <td>87.699934</td>\n      <td>0.865473</td>\n      <td>0.876271</td>\n      <td>0.008692</td>\n      <td>0.758931</td>\n      <td>0.011460</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.751302</td>\n      <td>67.984791</td>\n      <td>0.684068</td>\n      <td>0.410880</td>\n      <td>0.191540</td>\n      <td>72.575773</td>\n      <td>5.793029</td>\n      <td>83.027949</td>\n      <td>8.142960</td>\n      <td>0.828278</td>\n      <td>0.083659</td>\n      <td>0.727154</td>\n      <td>0.056903</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1.168189</td>\n      <td>49.581749</td>\n      <td>0.505397</td>\n      <td>0.611775</td>\n      <td>0.437051</td>\n      <td>66.671148</td>\n      <td>12.934831</td>\n      <td>74.474675</td>\n      <td>18.603783</td>\n      <td>0.739389</td>\n      <td>0.193005</td>\n      <td>0.669061</td>\n      <td>0.127240</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2.254341</td>\n      <td>18.631179</td>\n      <td>0.154145</td>\n      <td>0.886728</td>\n      <td>0.732925</td>\n      <td>57.278686</td>\n      <td>24.252981</td>\n      <td>65.782976</td>\n      <td>25.818213</td>\n      <td>0.640302</td>\n      <td>0.283215</td>\n      <td>0.570275</td>\n      <td>0.250451</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>6</td>\n      <td>1</td>\n      <td>12.516399</td>\n      <td>19.201521</td>\n      <td>0.186588</td>\n      <td>0.806154</td>\n      <td>0.706682</td>\n      <td>59.659431</td>\n      <td>23.200582</td>\n      <td>68.741819</td>\n      <td>24.977831</td>\n      <td>0.672284</td>\n      <td>0.273661</td>\n      <td>0.594345</td>\n      <td>0.239270</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>7</td>\n      <td>1</td>\n      <td>9.610622</td>\n      <td>20.152091</td>\n      <td>0.193252</td>\n      <td>0.750074</td>\n      <td>0.677493</td>\n      <td>61.345407</td>\n      <td>22.158502</td>\n      <td>70.762288</td>\n      <td>23.968632</td>\n      <td>0.694280</td>\n      <td>0.262520</td>\n      <td>0.611319</td>\n      <td>0.228306</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>8</td>\n      <td>1</td>\n      <td>8.661695</td>\n      <td>25.057034</td>\n      <td>0.219253</td>\n      <td>0.712597</td>\n      <td>0.647498</td>\n      <td>62.370921</td>\n      <td>21.092658</td>\n      <td>72.070202</td>\n      <td>22.899502</td>\n      <td>0.708747</td>\n      <td>0.250875</td>\n      <td>0.621693</td>\n      <td>0.217251</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>9</td>\n      <td>1</td>\n      <td>2.770374</td>\n      <td>25.893536</td>\n      <td>0.240637</td>\n      <td>0.730237</td>\n      <td>0.616622</td>\n      <td>61.641182</td>\n      <td>20.130258</td>\n      <td>71.050573</td>\n      <td>21.942100</td>\n      <td>0.699167</td>\n      <td>0.239762</td>\n      <td>0.614613</td>\n      <td>0.207203</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>10</td>\n      <td>1</td>\n      <td>2.178082</td>\n      <td>28.022814</td>\n      <td>0.224635</td>\n      <td>0.788966</td>\n      <td>0.616590</td>\n      <td>60.105156</td>\n      <td>19.802989</td>\n      <td>68.309591</td>\n      <td>22.646509</td>\n      <td>0.671101</td>\n      <td>0.245237</td>\n      <td>0.599881</td>\n      <td>0.203026</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>11</td>\n      <td>1</td>\n      <td>2.289579</td>\n      <td>19.125475</td>\n      <td>0.148035</td>\n      <td>0.912474</td>\n      <td>0.718543</td>\n      <td>56.219784</td>\n      <td>22.985344</td>\n      <td>64.405776</td>\n      <td>25.264968</td>\n      <td>0.627610</td>\n      <td>0.275711</td>\n      <td>0.557799</td>\n      <td>0.239801</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>0</td>\n      <td>2</td>\n      <td>5.828178</td>\n      <td>22.509506</td>\n      <td>0.191131</td>\n      <td>0.440478</td>\n      <td>0.017572</td>\n      <td>66.565125</td>\n      <td>0.532008</td>\n      <td>81.254468</td>\n      <td>0.825879</td>\n      <td>0.811308</td>\n      <td>0.008362</td>\n      <td>0.664787</td>\n      <td>0.005823</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5.978181</td>\n      <td>19.695817</td>\n      <td>0.150226</td>\n      <td>0.440467</td>\n      <td>0.014978</td>\n      <td>66.798837</td>\n      <td>0.596181</td>\n      <td>81.240863</td>\n      <td>0.712141</td>\n      <td>0.811321</td>\n      <td>0.007279</td>\n      <td>0.666611</td>\n      <td>0.007171</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3.269951</td>\n      <td>30.684411</td>\n      <td>0.281673</td>\n      <td>0.463411</td>\n      <td>0.036441</td>\n      <td>66.380344</td>\n      <td>1.100915</td>\n      <td>80.200721</td>\n      <td>1.653451</td>\n      <td>0.800896</td>\n      <td>0.016670</td>\n      <td>0.662219</td>\n      <td>0.011439</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0.945817</td>\n      <td>58.821293</td>\n      <td>0.588730</td>\n      <td>0.576282</td>\n      <td>0.198349</td>\n      <td>63.183201</td>\n      <td>5.635736</td>\n      <td>75.401871</td>\n      <td>8.446124</td>\n      <td>0.751580</td>\n      <td>0.086754</td>\n      <td>0.630598</td>\n      <td>0.055812</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1.097103</td>\n      <td>53.117871</td>\n      <td>0.538862</td>\n      <td>0.734099</td>\n      <td>0.362491</td>\n      <td>59.671546</td>\n      <td>8.654326</td>\n      <td>68.577224</td>\n      <td>15.622986</td>\n      <td>0.679587</td>\n      <td>0.163761</td>\n      <td>0.595991</td>\n      <td>0.085422</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2.280688</td>\n      <td>19.809886</td>\n      <td>0.171600</td>\n      <td>0.987963</td>\n      <td>0.657070</td>\n      <td>50.940175</td>\n      <td>21.130167</td>\n      <td>60.606331</td>\n      <td>22.831350</td>\n      <td>0.586325</td>\n      <td>0.256690</td>\n      <td>0.504586</td>\n      <td>0.219380</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>6</td>\n      <td>2</td>\n      <td>4.381033</td>\n      <td>23.840304</td>\n      <td>0.177178</td>\n      <td>0.911900</td>\n      <td>0.636284</td>\n      <td>53.140174</td>\n      <td>20.292666</td>\n      <td>63.435018</td>\n      <td>22.248174</td>\n      <td>0.617256</td>\n      <td>0.249467</td>\n      <td>0.527081</td>\n      <td>0.210461</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>7</td>\n      <td>2</td>\n      <td>5.283458</td>\n      <td>22.547529</td>\n      <td>0.157092</td>\n      <td>0.856075</td>\n      <td>0.613310</td>\n      <td>54.892082</td>\n      <td>19.546129</td>\n      <td>65.507924</td>\n      <td>21.526086</td>\n      <td>0.639975</td>\n      <td>0.241013</td>\n      <td>0.545022</td>\n      <td>0.202570</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>8</td>\n      <td>2</td>\n      <td>3.831899</td>\n      <td>32.091255</td>\n      <td>0.258347</td>\n      <td>0.813936</td>\n      <td>0.590458</td>\n      <td>56.451260</td>\n      <td>18.963056</td>\n      <td>67.051285</td>\n      <td>20.762889</td>\n      <td>0.657006</td>\n      <td>0.232313</td>\n      <td>0.560969</td>\n      <td>0.196379</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>9</td>\n      <td>2</td>\n      <td>3.871399</td>\n      <td>23.536122</td>\n      <td>0.176768</td>\n      <td>0.819556</td>\n      <td>0.560516</td>\n      <td>56.974183</td>\n      <td>18.059832</td>\n      <td>66.650866</td>\n      <td>19.739054</td>\n      <td>0.653655</td>\n      <td>0.220674</td>\n      <td>0.566869</td>\n      <td>0.187156</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>10</td>\n      <td>2</td>\n      <td>1.055093</td>\n      <td>56.425856</td>\n      <td>0.571810</td>\n      <td>0.867215</td>\n      <td>0.555382</td>\n      <td>56.200553</td>\n      <td>17.400038</td>\n      <td>64.408098</td>\n      <td>20.115798</td>\n      <td>0.630522</td>\n      <td>0.222801</td>\n      <td>0.559650</td>\n      <td>0.179962</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>11</td>\n      <td>2</td>\n      <td>2.296078</td>\n      <td>16.197719</td>\n      <td>0.164221</td>\n      <td>0.982693</td>\n      <td>0.655311</td>\n      <td>52.369068</td>\n      <td>20.976534</td>\n      <td>60.876323</td>\n      <td>22.543751</td>\n      <td>0.588664</td>\n      <td>0.254583</td>\n      <td>0.519160</td>\n      <td>0.218694</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.498692</td>\n      <td>79.193610</td>\n      <td>0.791973</td>\n      <td>0.402095</td>\n      <td>0.020770</td>\n      <td>71.527192</td>\n      <td>0.844426</td>\n      <td>82.760444</td>\n      <td>0.924812</td>\n      <td>0.826631</td>\n      <td>0.009277</td>\n      <td>0.714694</td>\n      <td>0.008890</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>1</td>\n      <td>3</td>\n      <td>7.262911</td>\n      <td>22.936478</td>\n      <td>0.176534</td>\n      <td>0.402557</td>\n      <td>0.015134</td>\n      <td>71.453508</td>\n      <td>0.706731</td>\n      <td>82.724424</td>\n      <td>0.688173</td>\n      <td>0.826210</td>\n      <td>0.006931</td>\n      <td>0.713868</td>\n      <td>0.007480</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.579616</td>\n      <td>74.971472</td>\n      <td>0.750021</td>\n      <td>0.420268</td>\n      <td>0.028524</td>\n      <td>70.831526</td>\n      <td>1.255325</td>\n      <td>81.968118</td>\n      <td>1.241464</td>\n      <td>0.818665</td>\n      <td>0.012422</td>\n      <td>0.707843</td>\n      <td>0.012323</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.780494</td>\n      <td>65.994675</td>\n      <td>0.662866</td>\n      <td>0.528486</td>\n      <td>0.189604</td>\n      <td>68.102729</td>\n      <td>4.863261</td>\n      <td>77.220625</td>\n      <td>8.315313</td>\n      <td>0.770146</td>\n      <td>0.084950</td>\n      <td>0.680688</td>\n      <td>0.048409</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2.030025</td>\n      <td>28.527957</td>\n      <td>0.255888</td>\n      <td>0.687300</td>\n      <td>0.360728</td>\n      <td>63.893221</td>\n      <td>9.516253</td>\n      <td>70.354955</td>\n      <td>15.642839</td>\n      <td>0.697595</td>\n      <td>0.164113</td>\n      <td>0.638587</td>\n      <td>0.095062</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>5</td>\n      <td>3</td>\n      <td>2.291271</td>\n      <td>12.248003</td>\n      <td>0.107747</td>\n      <td>0.950278</td>\n      <td>0.673963</td>\n      <td>55.585708</td>\n      <td>20.673400</td>\n      <td>62.361073</td>\n      <td>22.888275</td>\n      <td>0.604692</td>\n      <td>0.256348</td>\n      <td>0.546041</td>\n      <td>0.225549</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>30</td>\n      <td>6</td>\n      <td>3</td>\n      <td>5.458804</td>\n      <td>22.898440</td>\n      <td>0.178341</td>\n      <td>0.871154</td>\n      <td>0.653417</td>\n      <td>57.757004</td>\n      <td>19.867764</td>\n      <td>65.323740</td>\n      <td>22.401107</td>\n      <td>0.636881</td>\n      <td>0.250110</td>\n      <td>0.569011</td>\n      <td>0.216289</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>31</td>\n      <td>7</td>\n      <td>3</td>\n      <td>3.692444</td>\n      <td>32.027387</td>\n      <td>0.277362</td>\n      <td>0.816153</td>\n      <td>0.628325</td>\n      <td>59.153993</td>\n      <td>18.955033</td>\n      <td>67.345984</td>\n      <td>21.628257</td>\n      <td>0.659031</td>\n      <td>0.241200</td>\n      <td>0.583903</td>\n      <td>0.206184</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>8</td>\n      <td>3</td>\n      <td>4.457298</td>\n      <td>27.577025</td>\n      <td>0.209407</td>\n      <td>0.778215</td>\n      <td>0.602080</td>\n      <td>60.107457</td>\n      <td>18.077431</td>\n      <td>68.718008</td>\n      <td>20.760638</td>\n      <td>0.674229</td>\n      <td>0.231463</td>\n      <td>0.594184</td>\n      <td>0.196587</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>33</td>\n      <td>9</td>\n      <td>3</td>\n      <td>2.464701</td>\n      <td>30.315709</td>\n      <td>0.264350</td>\n      <td>0.790917</td>\n      <td>0.572708</td>\n      <td>59.624082</td>\n      <td>17.219157</td>\n      <td>67.984134</td>\n      <td>19.829306</td>\n      <td>0.667585</td>\n      <td>0.220603</td>\n      <td>0.590174</td>\n      <td>0.186964</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>34</td>\n      <td>10</td>\n      <td>3</td>\n      <td>1.924381</td>\n      <td>30.962343</td>\n      <td>0.267599</td>\n      <td>0.855967</td>\n      <td>0.583564</td>\n      <td>57.694033</td>\n      <td>17.522440</td>\n      <td>65.053532</td>\n      <td>21.057903</td>\n      <td>0.637320</td>\n      <td>0.231119</td>\n      <td>0.571981</td>\n      <td>0.187379</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>35</td>\n      <td>11</td>\n      <td>3</td>\n      <td>2.296966</td>\n      <td>18.372005</td>\n      <td>0.117269</td>\n      <td>0.974238</td>\n      <td>0.682670</td>\n      <td>53.926683</td>\n      <td>20.937119</td>\n      <td>61.466154</td>\n      <td>23.418681</td>\n      <td>0.596121</td>\n      <td>0.260191</td>\n      <td>0.531046</td>\n      <td>0.225240</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>36</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7.403112</td>\n      <td>16.964625</td>\n      <td>0.119128</td>\n      <td>0.454879</td>\n      <td>0.023495</td>\n      <td>67.117157</td>\n      <td>2.180876</td>\n      <td>80.671241</td>\n      <td>1.000569</td>\n      <td>0.805536</td>\n      <td>0.010144</td>\n      <td>0.668982</td>\n      <td>0.021197</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>37</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4.995937</td>\n      <td>24.724230</td>\n      <td>0.219541</td>\n      <td>0.449225</td>\n      <td>0.023501</td>\n      <td>67.725341</td>\n      <td>1.850006</td>\n      <td>80.889580</td>\n      <td>1.020508</td>\n      <td>0.807827</td>\n      <td>0.010274</td>\n      <td>0.675478</td>\n      <td>0.018249</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>38</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0.658782</td>\n      <td>72.917459</td>\n      <td>0.728823</td>\n      <td>0.462066</td>\n      <td>0.027778</td>\n      <td>67.479633</td>\n      <td>1.639547</td>\n      <td>80.359702</td>\n      <td>1.166481</td>\n      <td>0.802527</td>\n      <td>0.011716</td>\n      <td>0.673000</td>\n      <td>0.016211</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>39</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3.914467</td>\n      <td>19.969570</td>\n      <td>0.155608</td>\n      <td>0.572289</td>\n      <td>0.194547</td>\n      <td>64.378957</td>\n      <td>5.626853</td>\n      <td>75.627289</td>\n      <td>8.348169</td>\n      <td>0.753948</td>\n      <td>0.085678</td>\n      <td>0.642936</td>\n      <td>0.054567</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>40</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1.882785</td>\n      <td>27.615063</td>\n      <td>0.224291</td>\n      <td>0.756923</td>\n      <td>0.409478</td>\n      <td>59.305896</td>\n      <td>11.341373</td>\n      <td>67.597410</td>\n      <td>17.773932</td>\n      <td>0.669750</td>\n      <td>0.185568</td>\n      <td>0.593853</td>\n      <td>0.109781</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>41</td>\n      <td>5</td>\n      <td>4</td>\n      <td>2.257028</td>\n      <td>15.252948</td>\n      <td>0.135088</td>\n      <td>1.007771</td>\n      <td>0.674059</td>\n      <td>50.742770</td>\n      <td>21.892341</td>\n      <td>59.901167</td>\n      <td>23.655655</td>\n      <td>0.578482</td>\n      <td>0.265252</td>\n      <td>0.502270</td>\n      <td>0.228655</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>42</td>\n      <td>6</td>\n      <td>4</td>\n      <td>7.386781</td>\n      <td>17.991632</td>\n      <td>0.150818</td>\n      <td>0.921627</td>\n      <td>0.658823</td>\n      <td>53.315866</td>\n      <td>21.228828</td>\n      <td>63.148800</td>\n      <td>23.304230</td>\n      <td>0.613764</td>\n      <td>0.260370</td>\n      <td>0.528564</td>\n      <td>0.221316</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>43</td>\n      <td>7</td>\n      <td>4</td>\n      <td>6.499929</td>\n      <td>16.774439</td>\n      <td>0.146436</td>\n      <td>0.861984</td>\n      <td>0.636203</td>\n      <td>55.231389</td>\n      <td>20.495852</td>\n      <td>65.346234</td>\n      <td>22.563879</td>\n      <td>0.637829</td>\n      <td>0.251763</td>\n      <td>0.548161</td>\n      <td>0.213433</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>44</td>\n      <td>8</td>\n      <td>4</td>\n      <td>0.564435</td>\n      <td>75.275770</td>\n      <td>0.752554</td>\n      <td>0.818962</td>\n      <td>0.612088</td>\n      <td>56.703336</td>\n      <td>19.776725</td>\n      <td>66.907616</td>\n      <td>21.730683</td>\n      <td>0.655054</td>\n      <td>0.242347</td>\n      <td>0.563376</td>\n      <td>0.205875</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>45</td>\n      <td>9</td>\n      <td>4</td>\n      <td>4.230159</td>\n      <td>24.800304</td>\n      <td>0.194789</td>\n      <td>0.816713</td>\n      <td>0.580806</td>\n      <td>57.408929</td>\n      <td>18.887688</td>\n      <td>66.807388</td>\n      <td>20.621745</td>\n      <td>0.654885</td>\n      <td>0.229950</td>\n      <td>0.571124</td>\n      <td>0.196760</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>46</td>\n      <td>10</td>\n      <td>4</td>\n      <td>2.133507</td>\n      <td>32.293648</td>\n      <td>0.255840</td>\n      <td>0.869075</td>\n      <td>0.578446</td>\n      <td>56.383943</td>\n      <td>18.330126</td>\n      <td>64.336658</td>\n      <td>21.176163</td>\n      <td>0.629266</td>\n      <td>0.233965</td>\n      <td>0.561356</td>\n      <td>0.190386</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>47</td>\n      <td>11</td>\n      <td>4</td>\n      <td>2.289704</td>\n      <td>22.365919</td>\n      <td>0.122689</td>\n      <td>0.984491</td>\n      <td>0.673236</td>\n      <td>52.656545</td>\n      <td>21.565106</td>\n      <td>60.811820</td>\n      <td>23.408145</td>\n      <td>0.588282</td>\n      <td>0.262187</td>\n      <td>0.522276</td>\n      <td>0.224432</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>48</td>\n      <td>0</td>\n      <td>5</td>\n      <td>7.434802</td>\n      <td>24.381894</td>\n      <td>0.176671</td>\n      <td>0.379040</td>\n      <td>0.012830</td>\n      <td>72.055484</td>\n      <td>0.784554</td>\n      <td>83.932587</td>\n      <td>0.649882</td>\n      <td>0.838400</td>\n      <td>0.006527</td>\n      <td>0.720647</td>\n      <td>0.007492</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>49</td>\n      <td>1</td>\n      <td>5</td>\n      <td>0.514022</td>\n      <td>78.965386</td>\n      <td>0.789361</td>\n      <td>0.383768</td>\n      <td>0.013711</td>\n      <td>72.149370</td>\n      <td>0.810250</td>\n      <td>83.764348</td>\n      <td>0.655411</td>\n      <td>0.836791</td>\n      <td>0.006557</td>\n      <td>0.721526</td>\n      <td>0.008012</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>50</td>\n      <td>2</td>\n      <td>5</td>\n      <td>7.100224</td>\n      <td>13.427159</td>\n      <td>0.097930</td>\n      <td>0.404577</td>\n      <td>0.033809</td>\n      <td>72.014375</td>\n      <td>0.990010</td>\n      <td>82.838593</td>\n      <td>1.522677</td>\n      <td>0.827552</td>\n      <td>0.015227</td>\n      <td>0.720188</td>\n      <td>0.009959</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>51</td>\n      <td>3</td>\n      <td>5</td>\n      <td>3.982442</td>\n      <td>17.839483</td>\n      <td>0.140550</td>\n      <td>0.497902</td>\n      <td>0.164971</td>\n      <td>70.135079</td>\n      <td>3.399416</td>\n      <td>78.781306</td>\n      <td>7.195614</td>\n      <td>0.786153</td>\n      <td>0.073349</td>\n      <td>0.701487</td>\n      <td>0.033779</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>52</td>\n      <td>4</td>\n      <td>5</td>\n      <td>3.074195</td>\n      <td>17.230886</td>\n      <td>0.109658</td>\n      <td>0.667668</td>\n      <td>0.370346</td>\n      <td>65.889755</td>\n      <td>9.035906</td>\n      <td>71.387758</td>\n      <td>16.131524</td>\n      <td>0.708343</td>\n      <td>0.168921</td>\n      <td>0.659317</td>\n      <td>0.089739</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>53</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2.298211</td>\n      <td>5.325219</td>\n      <td>0.038804</td>\n      <td>0.935022</td>\n      <td>0.686798</td>\n      <td>57.961551</td>\n      <td>20.015809</td>\n      <td>62.994077</td>\n      <td>23.858967</td>\n      <td>0.612546</td>\n      <td>0.264160</td>\n      <td>0.570088</td>\n      <td>0.217341</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>54</td>\n      <td>6</td>\n      <td>5</td>\n      <td>4.851910</td>\n      <td>21.681248</td>\n      <td>0.157890</td>\n      <td>0.859018</td>\n      <td>0.662606</td>\n      <td>59.723316</td>\n      <td>19.031802</td>\n      <td>65.827806</td>\n      <td>23.156954</td>\n      <td>0.643240</td>\n      <td>0.255887</td>\n      <td>0.588873</td>\n      <td>0.206459</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>55</td>\n      <td>7</td>\n      <td>5</td>\n      <td>6.053369</td>\n      <td>12.818562</td>\n      <td>0.105500</td>\n      <td>0.806396</td>\n      <td>0.635335</td>\n      <td>61.026003</td>\n      <td>18.140484</td>\n      <td>67.747236</td>\n      <td>22.253673</td>\n      <td>0.664217</td>\n      <td>0.245756</td>\n      <td>0.602901</td>\n      <td>0.196729</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>56</td>\n      <td>8</td>\n      <td>5</td>\n      <td>7.487917</td>\n      <td>11.639407</td>\n      <td>0.079087</td>\n      <td>0.769866</td>\n      <td>0.607868</td>\n      <td>61.886208</td>\n      <td>17.275525</td>\n      <td>69.050499</td>\n      <td>21.303726</td>\n      <td>0.678644</td>\n      <td>0.235279</td>\n      <td>0.612221</td>\n      <td>0.187346</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>57</td>\n      <td>9</td>\n      <td>5</td>\n      <td>2.688849</td>\n      <td>29.478889</td>\n      <td>0.252266</td>\n      <td>0.787365</td>\n      <td>0.579229</td>\n      <td>61.261665</td>\n      <td>16.501380</td>\n      <td>68.035006</td>\n      <td>20.456959</td>\n      <td>0.669173</td>\n      <td>0.225175</td>\n      <td>0.606719</td>\n      <td>0.178565</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>58</td>\n      <td>10</td>\n      <td>5</td>\n      <td>1.193308</td>\n      <td>49.258273</td>\n      <td>0.504290</td>\n      <td>0.848962</td>\n      <td>0.585684</td>\n      <td>59.482351</td>\n      <td>16.710802</td>\n      <td>65.248626</td>\n      <td>21.403909</td>\n      <td>0.640269</td>\n      <td>0.233360</td>\n      <td>0.590158</td>\n      <td>0.178144</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>59</td>\n      <td>11</td>\n      <td>5</td>\n      <td>2.291784</td>\n      <td>27.729175</td>\n      <td>0.160606</td>\n      <td>0.967607</td>\n      <td>0.685043</td>\n      <td>55.896389</td>\n      <td>19.993290</td>\n      <td>61.596576</td>\n      <td>23.805249</td>\n      <td>0.598251</td>\n      <td>0.263349</td>\n      <td>0.550737</td>\n      <td>0.215149</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scikit learn best cfg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def calculate_statistics_sklearn(df: pd.DataFrame, model: str) -> Dict:\n",
    "    res = {'f1': mu_confidence_interval(df[df['model'] == model]['f1_test']),\n",
    "           'loss': mu_confidence_interval(df[df['model'] == model]['loss_test']),\n",
    "           'acc': mu_confidence_interval(df[df['model'] == model]['acc_test']),\n",
    "           'conf': df[df['model'] == model]['cfg'].unique()}\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tree based"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration random_forest_classifier mean metrics:\n",
      "f1_score: 0.609756989767005 ±0.006529683898881055\n",
      "loss: 0.39225757744490736 ±0.00712946750658302\n",
      "acc: 0.6077424225550925 ±0.007129467506583035\n",
      "\n",
      "Best hyperparams configuration:\n",
      "[\"{'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 700}\"]\n"
     ]
    }
   ],
   "source": [
    "res_random_forest = calculate_statistics_sklearn(tree_res, 'random_forest_classifier')\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_random_forest, 'random_forest_classifier')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decision tree"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration decision_tree_classifier mean metrics:\n",
      "f1_score: 0.6402913493313054 ±0.004216781363865863\n",
      "loss: 0.36008162828469237 ±0.004052145718922026\n",
      "acc: 0.6399183717153076 ±0.004052145718922028\n",
      "\n",
      "Best hyperparams configuration:\n",
      "[\"{'criterion': 'entropy', 'max_depth': 15}\"]\n"
     ]
    }
   ],
   "source": [
    "res_decision_tree = calculate_statistics_sklearn(tree_res, 'decision_tree_classifier')\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_decision_tree, 'decision_tree_classifier')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gaussian naive bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration gaussian_nb mean metrics:\n",
      "f1_score: 0.4526802764969494 ±0.010686051270129714\n",
      "loss: 0.5481864029029817 ±0.010409238716856704\n",
      "acc: 0.4518135970970182 ±0.010409238716856686\n",
      "\n",
      "Best hyperparams configuration:\n",
      "[\"{'var_smoothing': 8.111308307896872e-07}\"]\n"
     ]
    }
   ],
   "source": [
    "res_gaussian_nb = calculate_statistics_sklearn(naive_res, 'gaussian_nb')\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_gaussian_nb, 'gaussian_nb')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### QDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration qda mean metrics:\n",
      "f1_score: 0.5217235201683785 ±0.007963795942101407\n",
      "loss: 0.4649740608914607 ±0.00827922547607728\n",
      "acc: 0.5350259391085392 ±0.00827922547607726\n",
      "\n",
      "Best hyperparams configuration:\n",
      "[\"{'reg_param': 0.001, 'tol': 0.0001}\"]\n"
     ]
    }
   ],
   "source": [
    "res_qda = calculate_statistics_sklearn(naive_res, 'qda')\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_qda, 'qda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration svc mean metrics:\n",
      "f1_score: 0.8286206857647119 ±0.004227564469616818\n",
      "loss: 0.17076133850717423 ±0.004051273714411658\n",
      "acc: 0.8292386614928257 ±0.004051273714411657\n",
      "\n",
      "Best hyperparams configuration:\n",
      "[\"{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\"]\n"
     ]
    }
   ],
   "source": [
    "res_svm = calculate_statistics_sklearn(svm_res, 'svc')\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_svm, 'svc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "                      model     f1_mu    acc_mu   loss_mu     f1_ci    acc_ci  \\\n0                       mlp  0.861018  0.861018  0.861018  0.009781  0.009781   \n1  random_forest_classifier  0.609757  0.607742  0.392258  0.006530  0.007129   \n2  decision_tree_classifier  0.640291  0.639918  0.360082  0.004217  0.004052   \n3               gaussian_nb  0.452680  0.451814  0.548186  0.010686  0.010409   \n4                       qda  0.521724  0.535026  0.464974  0.007964  0.008279   \n5                       svc  0.828621  0.829239  0.170761  0.004228  0.004051   \n\n    loss_ci  \n0  0.009781  \n1  0.007129  \n2  0.004052  \n3  0.010409  \n4  0.008279  \n5  0.004051  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>f1_mu</th>\n      <th>acc_mu</th>\n      <th>loss_mu</th>\n      <th>f1_ci</th>\n      <th>acc_ci</th>\n      <th>loss_ci</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mlp</td>\n      <td>0.861018</td>\n      <td>0.861018</td>\n      <td>0.861018</td>\n      <td>0.009781</td>\n      <td>0.009781</td>\n      <td>0.009781</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>random_forest_classifier</td>\n      <td>0.609757</td>\n      <td>0.607742</td>\n      <td>0.392258</td>\n      <td>0.006530</td>\n      <td>0.007129</td>\n      <td>0.007129</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>decision_tree_classifier</td>\n      <td>0.640291</td>\n      <td>0.639918</td>\n      <td>0.360082</td>\n      <td>0.004217</td>\n      <td>0.004052</td>\n      <td>0.004052</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gaussian_nb</td>\n      <td>0.452680</td>\n      <td>0.451814</td>\n      <td>0.548186</td>\n      <td>0.010686</td>\n      <td>0.010409</td>\n      <td>0.010409</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>qda</td>\n      <td>0.521724</td>\n      <td>0.535026</td>\n      <td>0.464974</td>\n      <td>0.007964</td>\n      <td>0.008279</td>\n      <td>0.008279</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>svc</td>\n      <td>0.828621</td>\n      <td>0.829239</td>\n      <td>0.170761</td>\n      <td>0.004228</td>\n      <td>0.004051</td>\n      <td>0.004051</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: export DataFrame\n",
    "df_test_metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.86101756, 0.60975699, 0.64029135, 0.45268028, 0.52172352,\n",
      "       0.82862069]), array([0.86101756, 0.60774242, 0.63991837, 0.4518136 , 0.53502594,\n",
      "       0.82923866]), array([0.86101756, 0.39225758, 0.36008163, 0.5481864 , 0.46497406,\n",
      "       0.17076134])]\n",
      "[array([0.00978116, 0.00652968, 0.00421678, 0.01068605, 0.0079638 ,\n",
      "       0.00422756]), array([0.00978116, 0.00712947, 0.00405215, 0.01040924, 0.00827923,\n",
      "       0.00405127]), array([0.00978116, 0.00712947, 0.00405215, 0.01040924, 0.00827923,\n",
      "       0.00405127])]\n"
     ]
    }
   ],
   "source": [
    "from src.visualization.visualize import barplot_multiple_columns\n",
    "import numpy as np\n",
    "\n",
    "# TODO: change function and params name? Return new list or use reference (it works)?\n",
    "def add_value_array(old_list: List, df_test: pd.DataFrame, col_name: str) -> None:\n",
    "    tmp = []\n",
    "    for model_name in df_test['model'].unique():\n",
    "            metric_value = df_test[df_test['model'] == model_name][col_name].values[0]\n",
    "            tmp.append(metric_value)\n",
    "    old_list.append(np.array(tmp))\n",
    "\n",
    "metrics = []\n",
    "add_value_array(metrics, df_test_metric, 'f1_mu')\n",
    "add_value_array(metrics, df_test_metric, 'acc_mu')\n",
    "add_value_array(metrics, df_test_metric, 'loss_mu')\n",
    "print(metrics)\n",
    "\n",
    "\n",
    "y_errs = []\n",
    "add_value_array(y_errs, df_test_metric, 'f1_ci')\n",
    "add_value_array(y_errs, df_test_metric, 'acc_ci')\n",
    "add_value_array(y_errs, df_test_metric, 'loss_ci')\n",
    "print(y_errs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1152x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "barplot_multiple_columns(groups=df_test_metric['model'].unique(), elements_group=['f1', 'acc', 'loss'], data=metrics, yerr=y_errs, title='Test metrics')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}