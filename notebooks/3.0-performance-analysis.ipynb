{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Performance analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook all the results obtained will be reported, with particular reference to the best configuration for each model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os.path\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.models.config import param_layers, param_grid_mlp\n",
    "from src.utils.const import MODEL_RESULTS_CSV, NETWORK_RESULT_CSV\n",
    "from typing import Tuple\n",
    "\n",
    "from src.utils.util_models import get_best_configuration_mlp\n",
    "from src.visualization.visualize import barplot_multiple_columns\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Useful path to data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "RESULTS_FOLDER = os.path.join('..', MODEL_RESULTS_CSV)\n",
    "MLP_RESULTS_FOLDER = os.path.join('..', NETWORK_RESULT_CSV)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read output csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "mlp_all = pd.read_csv(os.path.join(MLP_RESULTS_FOLDER, 'out_mlp_all.csv'))\n",
    "mlp_batch_bad = pd.read_csv(os.path.join(MLP_RESULTS_FOLDER, 'out_bad_config_batch.csv'))\n",
    "mlp_batch = pd.read_csv(os.path.join(MLP_RESULTS_FOLDER, 'out_mlp_batch.csv'))\n",
    "\n",
    "svm_res = pd.read_csv(os.path.join(RESULTS_FOLDER, 'out_svm.csv'))\n",
    "naive_res = pd.read_csv(os.path.join(RESULTS_FOLDER, 'best_out_naive_bayes.csv'))\n",
    "tree_res = pd.read_csv(os.path.join(RESULTS_FOLDER, 'best_out_tree_based.csv'))\n",
    "\n",
    "svm_val = pd.read_csv(os.path.join(RESULTS_FOLDER, 'out_grid_svm.csv'))\n",
    "naive_val = pd.read_csv(os.path.join(RESULTS_FOLDER, 'out_grid_naive_bayes.csv'))\n",
    "tree_val = pd.read_csv(os.path.join(RESULTS_FOLDER, 'out_grid_tree_based.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_metric = pd.DataFrame()\n",
    "df_train_metric=pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def mu_confidence_interval(data: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute mean and t_student from a np.ndarray\n",
    "    :param data: the DataFrame that contains movies_id and tmdb_id\n",
    "    :return: Dict with metric values\n",
    "    \"\"\"\n",
    "    t = 2.13\n",
    "    mu = np.mean(data)\n",
    "    standard_deviation = np.std(data)\n",
    "    M = data.shape[0]\n",
    "    t_student = t * standard_deviation / np.sqrt(M)\n",
    "    first_interval = mu - t_student\n",
    "    second_interval = mu + t_student\n",
    "    return {\n",
    "        'mu': mu,\n",
    "        't_student': t_student,\n",
    "        'first_interval': first_interval,\n",
    "        'second_interval': second_interval\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def summary_statistics_model(df_score: pd.DataFrame, dict_: Dict, model: str, train: bool = False) -> pd.DataFrame:\n",
    "    if not train:\n",
    "        print(\n",
    "            f\"Best configuration {model} mean metrics:\\n\"\n",
    "            f\"f1_score: {dict_['f1']['mu']} ±{dict_['f1']['t_student']}\\n\"\n",
    "            f\"loss: {dict_['loss']['mu']} ±{dict_['loss']['t_student']}\\n\"\n",
    "            f\"acc: {dict_['acc']['mu']} ±{dict_['acc']['t_student']}\\n\\n\"\n",
    "            f\"Best hyperparams configuration:\"\n",
    "        )\n",
    "        if model == \"mlp\" or model=='MLP':\n",
    "            best_cfg_mlp_all = get_best_configuration_mlp(int(dict_['conf']), param_layers, param_grid_mlp)\n",
    "            for idx, key in enumerate(param_layers.keys()):\n",
    "                print(f\"{key}: {best_cfg_mlp_all[idx]}\")\n",
    "            for idx, key in enumerate(param_grid_mlp.keys(), 7):\n",
    "                print(f\"{key}: {best_cfg_mlp_all[idx]}\")\n",
    "        else:\n",
    "            print(f\"{dict_['conf']}\")\n",
    "\n",
    "        new_test_score = pd.DataFrame({\n",
    "            'model': [model],\n",
    "            'f1_mu': [dict_['f1']['mu']],\n",
    "            'acc_mu': [dict_['acc']['mu']],\n",
    "            'loss_mu': [dict_['loss']['mu']],\n",
    "            'f1_ci': [dict_['f1']['t_student']],\n",
    "            'acc_ci': [dict_['acc']['t_student']],\n",
    "            'loss_ci': [dict_['loss']['t_student']],\n",
    "        })\n",
    "        df_score = pd.concat([df_score, new_test_score], ignore_index=True)\n",
    "    else:\n",
    "        print(\n",
    "            f\"Best configuration {model} mean metrics:\\n\"\n",
    "            f\"train f1: {dict_['train_score']['mu']} ±{dict_['train_score']['t_student']}\\n\"\n",
    "            f\"validation f1: {dict_['val_score']['mu']} ±{dict_['val_score']['t_student']}\\n\"\n",
    "        )\n",
    "        new_test_score = pd.DataFrame({\n",
    "            'model': [model],\n",
    "            'train_score': [dict_['train_score']['mu']],\n",
    "            'val_score': [dict_['val_score']['mu']],\n",
    "            'train_ci': [dict_['train_score']['t_student']],\n",
    "            'val_ci': [dict_['val_score']['t_student']]\n",
    "        })\n",
    "        df_score = pd.concat([df_score, new_test_score], ignore_index=True)\n",
    "    return df_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_max_f1_cfg(df: pd.DataFrame) -> List:\n",
    "    \"\"\"\n",
    "    Find best configuration based on f1-score\n",
    "    :param data: the DataFrame that contains model outputs\n",
    "    :return: List with best configuration indeces\n",
    "    \"\"\"\n",
    "    cfg = []\n",
    "    for fold in df['fold'].unique():\n",
    "        idx = df[df['fold'] == fold]['f1_test'].idxmax()\n",
    "        cfg.append(df.iloc[idx]['cfg'])\n",
    "    cfgs = np.unique(np.array(cfg))\n",
    "    return cfgs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_best_conf(lst_conf, df: pd.DataFrame) -> Dict:\n",
    "    conf = []\n",
    "    for idx, cfg in enumerate(lst_conf):\n",
    "        conf.append(\n",
    "            {\n",
    "                'f1': mu_confidence_interval(df[df['cfg'] == cfg]['f1_test']),\n",
    "                'loss': mu_confidence_interval(df[df['cfg'] == cfg]['loss_test']),\n",
    "                'acc': mu_confidence_interval(df[df['cfg'] == cfg]['acc_test'])\n",
    "            }\n",
    "        )\n",
    "        conf[idx]['conf'] = cfg\n",
    "        conf[idx]['acc']['mu'] /= 100\n",
    "        conf[idx]['acc']['t_student'] /= 100\n",
    "    max = conf[0]\n",
    "    for elm in conf:\n",
    "        if max['f1']['mu'] < elm['f1']['mu'] and max['f1']['t_student'] > elm['f1']['t_student']:\n",
    "            max = elm\n",
    "    return max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_statistics_sklearn(df: pd.DataFrame, model: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a well-formatted Dict that contains the metrics\n",
    "    :param df: the DataFrame that contains metrics\n",
    "    :return: well-formatted Dict\n",
    "    \"\"\"\n",
    "    res = {'f1': mu_confidence_interval(df[df['model'] == model]['f1_test']),\n",
    "           'loss': mu_confidence_interval(df[df['model'] == model]['loss_test']),\n",
    "           'acc': mu_confidence_interval(df[df['model'] == model]['acc_test']),\n",
    "           'conf': df[df['model'] == model]['cfg'].unique()}\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_statistics_sklearn_train(df: pd.DataFrame, first:bool=True) -> Dict:\n",
    "    if first:\n",
    "        res = {'train_score': mu_confidence_interval(df['mean_train_score'].iloc[0:5]),\n",
    "               'val_score': mu_confidence_interval(df['mean_test_score'].iloc[0:5]),\n",
    "               }\n",
    "    else:\n",
    "        res = {'train_score': mu_confidence_interval(df['mean_train_score'].iloc[5:10]),\n",
    "               'val_score': mu_confidence_interval(df['mean_test_score'].iloc[5:10]),\n",
    "               }\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_statistics_mlp_train(cfg:int, df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a well-formatted Dict that contains the metrics\n",
    "    :param cfg: the configuration index\n",
    "    :param cfg: the DataFrame that contains metric\n",
    "    :return: well-formatted Dict\n",
    "    \"\"\"\n",
    "    res = {'train_score': mu_confidence_interval(df[df['cfg'] == cfg]['mean_f1_train']),\n",
    "               'val_score': mu_confidence_interval(df[df['cfg'] == cfg]['mean_f1_val']),\n",
    "               }\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_value_array(old_list: List, df_test: pd.DataFrame, col_name: str, last_idx: int) -> None:\n",
    "    for model_name in df_test['model'].unique():\n",
    "        if col_name == 'metrics':\n",
    "            metric_value = df_test[df_test['model'] == model_name].iloc[:, 1:last_idx].iloc[0]\n",
    "        else:\n",
    "            metric_value = df_test[df_test['model'] == model_name].iloc[:, last_idx:].iloc[0]\n",
    "\n",
    "        old_list.append(np.array(metric_value))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Work with results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scikit-learn models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### RandomForestClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_random_forest_train = calculate_statistics_sklearn_train(tree_val)\n",
    "df_train_metric = summary_statistics_model(df_train_metric, res_random_forest_train, 'Random forest classifier', train=True)\n",
    "\n",
    "res_random_forest = calculate_statistics_sklearn(tree_res, 'random_forest_classifier')\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_random_forest, 'Random forest classifier')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DecisionTreeClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_decision_tree_train = calculate_statistics_sklearn_train(tree_val,first=False)\n",
    "df_train_metric = summary_statistics_model(df_train_metric, res_decision_tree_train, 'Decision tree classifier', train=True)\n",
    "\n",
    "res_decision_tree = calculate_statistics_sklearn(tree_res, 'decision_tree_classifier')\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_decision_tree, 'Decision tree classifier')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GaussianNB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_gaussian_nb_train = calculate_statistics_sklearn_train(naive_val)\n",
    "df_train_metric = summary_statistics_model(df_train_metric, res_gaussian_nb_train, 'GaussianNB', train=True)\n",
    "\n",
    "res_gaussian_nb = calculate_statistics_sklearn(naive_res, 'gaussian_nb')\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_gaussian_nb, 'GaussianNB')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### QDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_qda_train = calculate_statistics_sklearn_train(naive_val,first=False)\n",
    "df_train_metric = summary_statistics_model(df_train_metric, res_qda_train, 'QDA', train=True)\n",
    "\n",
    "res_qda = calculate_statistics_sklearn(naive_res, 'qda')\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_qda, 'QDA')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_svm_train = calculate_statistics_sklearn_train(svm_val)\n",
    "df_train_metric = summary_statistics_model(df_train_metric, res_svm_train, 'SVM', train=True)\n",
    "\n",
    "res_svm = calculate_statistics_sklearn(svm_res, 'svc')\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_svm, 'SVM')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MovieNet (MLP)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_cfg = find_max_f1_cfg(mlp_all)\n",
    "print(f'Indices of the best configurations: {best_cfg}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_mlp_train = calculate_statistics_mlp_train(3,mlp_all)\n",
    "df_train_metric = summary_statistics_model(df_train_metric, res_mlp_train, 'MLP', train=True)\n",
    "\n",
    "res_mlp = find_best_conf(best_cfg, mlp_all)\n",
    "df_test_metric = summary_statistics_model(df_test_metric, res_mlp, \"MLP\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train & Validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = []\n",
    "y_errs = []\n",
    "add_value_array(metrics, df_train_metric, 'metrics', 3)\n",
    "add_value_array(y_errs, df_train_metric, 'interval', 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "barplot_multiple_columns(groups=['Training', 'Validation'],\n",
    "                         elements_group=df_train_metric['model'].unique(), data=metrics, yerr=y_errs,\n",
    "                         title='Train/val metrics',\n",
    "                         upper_title=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = []\n",
    "y_errs = []\n",
    "add_value_array(metrics, df_test_metric, 'metrics', 4)\n",
    "add_value_array(y_errs, df_test_metric, 'interval', 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "barplot_multiple_columns(groups=['F1 score ', 'Accuracy', 'Loss (lower is better)'],\n",
    "                         elements_group=df_test_metric['model'].unique(), data=metrics, yerr=y_errs,\n",
    "                         title='Test metrics',\n",
    "                         upper_title=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### working with batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index  batch_size   f1_mean  f1_confidence  loss_mean  loss_confidence  \\\n",
      "0       0           8  0.285382       0.242427   8.021037         5.736820   \n",
      "1       1          16  0.310350       0.229459   6.453717         4.000400   \n",
      "2       2          32  0.400204       0.270151   5.942804         6.218700   \n",
      "3       3          64  0.446364       0.233992   2.074904         1.458697   \n",
      "4       4         128  0.326819       0.159141   1.850459         0.682113   \n",
      "5       5         256  0.121477       0.044247   2.276308         0.016917   \n",
      "6       6         512  0.170163       0.012856   6.918986         2.838452   \n",
      "7       7        1024  0.175928       0.055172   6.227965         1.850419   \n",
      "8       8        2048  0.303729       0.221356   5.000649         2.725434   \n",
      "9       9        4096  0.225762       0.032366   3.205096         0.673349   \n",
      "10     10        8192  0.364835       0.136903   1.696874         0.454730   \n",
      "11     11       16384  0.142564       0.018369   2.292822         0.002985   \n",
      "\n",
      "     acc_mean  acc_confidence  \n",
      "0   31.332360       23.087817  \n",
      "1   33.827120       21.553753  \n",
      "2   40.886792       26.451277  \n",
      "3   46.121962       21.376658  \n",
      "4   35.214705       13.149677  \n",
      "5   14.253447        4.945503  \n",
      "6   21.122629        2.101917  \n",
      "7   20.864002        6.164735  \n",
      "8   34.328098       20.554320  \n",
      "9   26.804912        2.520606  \n",
      "10  39.392587       10.761874  \n",
      "11  20.758058        3.818276  \n"
     ]
    }
   ],
   "source": [
    "from src.models.config import param_layers_batch, param_grid_mlp_batch\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for idx in mlp_batch['cfg'].unique():\n",
    "    config = get_best_configuration_mlp(idx, param_layers_batch, param_grid_mlp_batch)\n",
    "    batch_size = config[9]\n",
    "    f1_dict = mu_confidence_interval(mlp_batch[mlp_batch['cfg'] == idx]['f1_test'])\n",
    "    loss_dict = mu_confidence_interval(mlp_batch[mlp_batch['cfg'] == idx]['loss_test'])\n",
    "    acc_dict = mu_confidence_interval(mlp_batch[mlp_batch['cfg'] == idx]['acc_test'])\n",
    "\n",
    "    new_sample = pd.DataFrame({\n",
    "        'index': [idx],\n",
    "        'batch_size': [batch_size],\n",
    "        'f1_mean': [f1_dict['mu']],\n",
    "        'f1_confidence': [f1_dict['t_student']],\n",
    "        'loss_mean': [loss_dict['mu']],\n",
    "        'loss_confidence': [loss_dict['t_student']],\n",
    "        'acc_mean': [acc_dict['mu']],\n",
    "        'acc_confidence': [acc_dict['t_student']]\n",
    "\n",
    "    })\n",
    "    df = pd.concat([df, new_sample], ignore_index=True)\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "47cc0a8402cc523f6ba566443af0567aa44ce511868ca4db7d50606c33fb677f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}