{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Read results datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook all the results obtained will be reported, with particular reference to the best configuration for each model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os.path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.models.config import param_layers, param_grid_mlp, param_layers_batch, param_grid_mlp_batch\n",
    "from src.utils.const import MODEL_RESULTS_CSV, NETWORK_RESULT_CSV\n",
    "from typing import Tuple"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Useful path to data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "RESULTS_FOLDER = os.path.join('..', MODEL_RESULTS_CSV)\n",
    "MLP_RESULTS_FOLDER = os.path.join('..', NETWORK_RESULT_CSV)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read output csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "mlp_all = pd.read_csv(os.path.join(MLP_RESULTS_FOLDER, 'out_mlp_all.csv'))\n",
    "mlp_batch = pd.read_csv(os.path.join(MLP_RESULTS_FOLDER, 'out_mlp_batch.csv'))\n",
    "svm_res = pd.read_csv(os.path.join(RESULTS_FOLDER, 'out_svm.csv'))\n",
    "naive_res = pd.read_csv(os.path.join(RESULTS_FOLDER, 'best_out_naive_bayes.csv'))\n",
    "tree_res = pd.read_csv(os.path.join(RESULTS_FOLDER, 'best_out_tree_based.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find best configuration of MLP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utility function to explore results DataFrame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to find configuration with the best f1-score for each fold."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def find_max_f1_cfg(df: pd.DataFrame) -> List:\n",
    "    cfg = []\n",
    "    for fold in df['fold'].unique():\n",
    "        idx = df[df['fold'] == fold]['f1_test'].idxmax()\n",
    "        cfg.append(df.iloc[idx]['cfg'])\n",
    "    cfgs = np.unique(np.array(cfg))\n",
    "    return cfgs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration ID: [ 3. 17. 18. 35.]\n"
     ]
    }
   ],
   "source": [
    "best_cfg = find_max_f1_cfg(mlp_all)\n",
    "print(f'Best configuration ID: {best_cfg}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having performed cross validation with several test sets, it is possible to obtain the mean value of the specified metric and its confidence interval, with 90% accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def mu_confidence_interval(data: np.ndarray) -> {}:\n",
    "    t = 1.64\n",
    "    mu = np.mean(data)\n",
    "    standard_deviation = np.std(data)\n",
    "    M = data.shape[0]\n",
    "    t_student = t * standard_deviation / np.sqrt(M)\n",
    "    first_interval = mu - t_student\n",
    "    second_interval = mu + t_student\n",
    "    return {\n",
    "        'mu': mu,\n",
    "        't_student': t_student,\n",
    "        'first_interval': first_interval,\n",
    "        'second_interval': second_interval\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to find the best configuration between the indexes that we found previously, this function calculate the mean on each configuration between the folds, and select the one which has the higher mean."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def find_best_conf(lst_conf, df: pd.DataFrame) -> dict:\n",
    "    conf = []\n",
    "    for idx, cfg in enumerate(lst_conf):\n",
    "        conf.append(\n",
    "            {\n",
    "                'f1': mu_confidence_interval(df[df['cfg'] == cfg]['f1_test']),\n",
    "                'loss': mu_confidence_interval(df[df['cfg'] == cfg]['f1_test']),\n",
    "                'acc': mu_confidence_interval(df[df['cfg'] == cfg]['f1_test'])\n",
    "            }\n",
    "        )\n",
    "        conf[idx]['conf'] = cfg\n",
    "    max = conf[0]\n",
    "\n",
    "    for elm in conf:\n",
    "        if max['f1']['mu'] < elm['f1']['mu']:\n",
    "            max = elm\n",
    "    return max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since in the output files containing the results of the neural network, the configuration is stored via the index, it is necessary to recalculate all configurations and select only the one of interest, specifying the index."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_best_configuration_mlp(cfg: int, p_layer, p_grid_mlp) -> Tuple:\n",
    "    hyper_parameters_model_all = itertools.product(\n",
    "        p_layer['input_act'],\n",
    "        p_layer['hidden_act'],\n",
    "        p_layer['hidden_size'],\n",
    "        p_layer['num_hidden_layers'],\n",
    "        p_layer['dropout'],\n",
    "        p_layer['batch_norm'],\n",
    "        p_layer['output_fn'],\n",
    "        p_grid_mlp['num_epochs'],\n",
    "        p_grid_mlp['starting_lr'],\n",
    "        p_grid_mlp['batch_size'],\n",
    "        p_grid_mlp['optim'],\n",
    "        p_grid_mlp['momentum'],\n",
    "        p_grid_mlp['weight_decay'],\n",
    "    )\n",
    "    return list(hyper_parameters_model_all)[cfg]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utility function to print the calculated statistics and the relative configuration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def print_statistics_model(dictionary: Dict, model: str):\n",
    "    print(\n",
    "        f\"Best configuration {model} mean metrics:\\n\"\n",
    "        f\"f1_score: {dictionary['f1']['mu']} ±{dictionary['f1']['t_student']}\\n\"\n",
    "        f\"loss: {dictionary['loss']['mu']} ±{dictionary['loss']['t_student']}\\n\"\n",
    "        f\"acc: {dictionary['acc']['mu']} ±{dictionary['acc']['t_student']}\\n\\n\"\n",
    "        f\"Best hyperparams configuration:\"\n",
    "    )\n",
    "    if model == \"MLP\":\n",
    "        best_cfg_mlp_all = get_best_configuration_mlp(int(dictionary['conf']), param_layers, param_grid_mlp)\n",
    "        for idx, key in enumerate(param_layers.keys()):\n",
    "            print(f\"{key}: {best_cfg_mlp_all[idx]}\")\n",
    "        for idx, key in enumerate(param_grid_mlp.keys(), 7):\n",
    "            print(f\"{key}: {best_cfg_mlp_all[idx]}\")\n",
    "    else:\n",
    "        print(f\"{dictionary['conf']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results best cfg mlp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration MLP mean metrics:\n",
      "f1_score: 0.8610175579840659 ±0.007531035084099725\n",
      "loss: 0.8610175579840659 ±0.007531035084099725\n",
      "acc: 0.8610175579840659 ±0.007531035084099725\n",
      "\n",
      "Best hyperparams configuration:\n",
      "input_act: LeakyReLU(negative_slope=0.01)\n",
      "hidden_act: LeakyReLU(negative_slope=0.01)\n",
      "hidden_size: 512\n",
      "num_hidden_layers: 3\n",
      "dropout: 0.2\n",
      "batch_norm: True\n",
      "output_fn: None\n",
      "num_epochs: 200\n",
      "starting_lr: 0.001\n",
      "batch_size: 128\n",
      "optim: <class 'torch.optim.adam.Adam'>\n",
      "momentum: 0.9\n",
      "weight_decay: 1e-05\n"
     ]
    }
   ],
   "source": [
    "res_mlp = find_best_conf(best_cfg, mlp_all)\n",
    "print_statistics_model(res_mlp, \"MLP\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### mlp with different batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "    Unnamed: 0  cfg  fold  loss_test   acc_test   f1_test  mean_loss  \\\n0            0    0     1  18.940403  13.612167  0.148006   0.289398   \n1            1    1     1  13.517536  22.813688  0.216087   0.296447   \n2            2    2     1  18.105448  12.433460  0.142572   0.301010   \n3            3    3     1   0.751302  67.984791  0.684068   0.410880   \n4            4    4     1   1.168189  49.581749  0.505397   0.611775   \n5            5    5     1   2.254341  18.631179  0.154145   0.886728   \n6            6    6     1  12.516399  19.201521  0.186588   0.806154   \n7            7    7     1   9.610622  20.152091  0.193252   0.750074   \n8            8    8     1   8.661695  25.057034  0.219253   0.712597   \n9            9    9     1   2.770374  25.893536  0.240637   0.730237   \n10          10   10     1   2.178082  28.022814  0.224635   0.788966   \n11          11   11     1   2.289579  19.125475  0.148035   0.912474   \n12          12    0     2   5.828178  22.509506  0.191131   0.440478   \n13          13    1     2   5.978181  19.695817  0.150226   0.440467   \n14          14    2     2   3.269951  30.684411  0.281673   0.463411   \n15          15    3     2   0.945817  58.821293  0.588730   0.576282   \n16          16    4     2   1.097103  53.117871  0.538862   0.734099   \n17          17    5     2   2.280688  19.809886  0.171600   0.987963   \n18          18    6     2   4.381033  23.840304  0.177178   0.911900   \n19          19    7     2   5.283458  22.547529  0.157092   0.856075   \n20          20    8     2   3.831899  32.091255  0.258347   0.813936   \n21          21    9     2   3.871399  23.536122  0.176768   0.819556   \n22          22   10     2   1.055093  56.425856  0.571810   0.867215   \n23          23   11     2   2.296078  16.197719  0.164221   0.982693   \n24          24    0     3   0.498692  79.193610  0.791973   0.402095   \n25          25    1     3   7.262911  22.936478  0.176534   0.402557   \n26          26    2     3   0.579616  74.971472  0.750021   0.420268   \n27          27    3     3   0.780494  65.994675  0.662866   0.528486   \n28          28    4     3   2.030025  28.527957  0.255888   0.687300   \n29          29    5     3   2.291271  12.248003  0.107747   0.950278   \n30          30    6     3   5.458804  22.898440  0.178341   0.871154   \n31          31    7     3   3.692444  32.027387  0.277362   0.816153   \n32          32    8     3   4.457298  27.577025  0.209407   0.778215   \n33          33    9     3   2.464701  30.315709  0.264350   0.790917   \n34          34   10     3   1.924381  30.962343  0.267599   0.855967   \n35          35   11     3   2.296966  18.372005  0.117269   0.974238   \n36          36    0     4   7.403112  16.964625  0.119128   0.454879   \n37          37    1     4   4.995937  24.724230  0.219541   0.449225   \n38          38    2     4   0.658782  72.917459  0.728823   0.462066   \n39          39    3     4   3.914467  19.969570  0.155608   0.572289   \n40          40    4     4   1.882785  27.615063  0.224291   0.756923   \n41          41    5     4   2.257028  15.252948  0.135088   1.007771   \n42          42    6     4   7.386781  17.991632  0.150818   0.921627   \n43          43    7     4   6.499929  16.774439  0.146436   0.861984   \n44          44    8     4   0.564435  75.275770  0.752554   0.818962   \n45          45    9     4   4.230159  24.800304  0.194789   0.816713   \n46          46   10     4   2.133507  32.293648  0.255840   0.869075   \n47          47   11     4   2.289704  22.365919  0.122689   0.984491   \n48          48    0     5   7.434802  24.381894  0.176671   0.379040   \n49          49    1     5   0.514022  78.965386  0.789361   0.383768   \n50          50    2     5   7.100224  13.427159  0.097930   0.404577   \n51          51    3     5   3.982442  17.839483  0.140550   0.497902   \n52          52    4     5   3.074195  17.230886  0.109658   0.667668   \n53          53    5     5   2.298211   5.325219  0.038804   0.935022   \n54          54    6     5   4.851910  21.681248  0.157890   0.859018   \n55          55    7     5   6.053369  12.818562  0.105500   0.806396   \n56          56    8     5   7.487917  11.639407  0.079087   0.769866   \n57          57    9     5   2.688849  29.478889  0.252266   0.787365   \n58          58   10     5   1.193308  49.258273  0.504290   0.848962   \n59          59   11     5   2.291784  27.729175  0.160606   0.967607   \n\n    std_loss  mean_acc_val  std_acc_val  mean_acc_train  std_acc_train  \\\n0   0.013900     75.375842     0.475101       88.254775       0.562407   \n1   0.022876     75.671540     1.257495       87.909500       0.968884   \n2   0.020293     75.823471     1.143876       87.699934       0.865473   \n3   0.191540     72.575773     5.793029       83.027949       8.142960   \n4   0.437051     66.671148    12.934831       74.474675      18.603783   \n5   0.732925     57.278686    24.252981       65.782976      25.818213   \n6   0.706682     59.659431    23.200582       68.741819      24.977831   \n7   0.677493     61.345407    22.158502       70.762288      23.968632   \n8   0.647498     62.370921    21.092658       72.070202      22.899502   \n9   0.616622     61.641182    20.130258       71.050573      21.942100   \n10  0.616590     60.105156    19.802989       68.309591      22.646509   \n11  0.718543     56.219784    22.985344       64.405776      25.264968   \n12  0.017572     66.565125     0.532008       81.254468       0.825879   \n13  0.014978     66.798837     0.596181       81.240863       0.712141   \n14  0.036441     66.380344     1.100915       80.200721       1.653451   \n15  0.198349     63.183201     5.635736       75.401871       8.446124   \n16  0.362491     59.671546     8.654326       68.577224      15.622986   \n17  0.657070     50.940175    21.130167       60.606331      22.831350   \n18  0.636284     53.140174    20.292666       63.435018      22.248174   \n19  0.613310     54.892082    19.546129       65.507924      21.526086   \n20  0.590458     56.451260    18.963056       67.051285      20.762889   \n21  0.560516     56.974183    18.059832       66.650866      19.739054   \n22  0.555382     56.200553    17.400038       64.408098      20.115798   \n23  0.655311     52.369068    20.976534       60.876323      22.543751   \n24  0.020770     71.527192     0.844426       82.760444       0.924812   \n25  0.015134     71.453508     0.706731       82.724424       0.688173   \n26  0.028524     70.831526     1.255325       81.968118       1.241464   \n27  0.189604     68.102729     4.863261       77.220625       8.315313   \n28  0.360728     63.893221     9.516253       70.354955      15.642839   \n29  0.673963     55.585708    20.673400       62.361073      22.888275   \n30  0.653417     57.757004    19.867764       65.323740      22.401107   \n31  0.628325     59.153993    18.955033       67.345984      21.628257   \n32  0.602080     60.107457    18.077431       68.718008      20.760638   \n33  0.572708     59.624082    17.219157       67.984134      19.829306   \n34  0.583564     57.694033    17.522440       65.053532      21.057903   \n35  0.682670     53.926683    20.937119       61.466154      23.418681   \n36  0.023495     67.117157     2.180876       80.671241       1.000569   \n37  0.023501     67.725341     1.850006       80.889580       1.020508   \n38  0.027778     67.479633     1.639547       80.359702       1.166481   \n39  0.194547     64.378957     5.626853       75.627289       8.348169   \n40  0.409478     59.305896    11.341373       67.597410      17.773932   \n41  0.674059     50.742770    21.892341       59.901167      23.655655   \n42  0.658823     53.315866    21.228828       63.148800      23.304230   \n43  0.636203     55.231389    20.495852       65.346234      22.563879   \n44  0.612088     56.703336    19.776725       66.907616      21.730683   \n45  0.580806     57.408929    18.887688       66.807388      20.621745   \n46  0.578446     56.383943    18.330126       64.336658      21.176163   \n47  0.673236     52.656545    21.565106       60.811820      23.408145   \n48  0.012830     72.055484     0.784554       83.932587       0.649882   \n49  0.013711     72.149370     0.810250       83.764348       0.655411   \n50  0.033809     72.014375     0.990010       82.838593       1.522677   \n51  0.164971     70.135079     3.399416       78.781306       7.195614   \n52  0.370346     65.889755     9.035906       71.387758      16.131524   \n53  0.686798     57.961551    20.015809       62.994077      23.858967   \n54  0.662606     59.723316    19.031802       65.827806      23.156954   \n55  0.635335     61.026003    18.140484       67.747236      22.253673   \n56  0.607868     61.886208    17.275525       69.050499      21.303726   \n57  0.579229     61.261665    16.501380       68.035006      20.456959   \n58  0.585684     59.482351    16.710802       65.248626      21.403909   \n59  0.685043     55.896389    19.993290       61.596576      23.805249   \n\n    mean_f1_train  std_f1_train  mean_f1_val  std_f1_val  \n0        0.881785      0.005654     0.754430    0.003962  \n1        0.878310      0.009760     0.757455    0.012299  \n2        0.876271      0.008692     0.758931    0.011460  \n3        0.828278      0.083659     0.727154    0.056903  \n4        0.739389      0.193005     0.669061    0.127240  \n5        0.640302      0.283215     0.570275    0.250451  \n6        0.672284      0.273661     0.594345    0.239270  \n7        0.694280      0.262520     0.611319    0.228306  \n8        0.708747      0.250875     0.621693    0.217251  \n9        0.699167      0.239762     0.614613    0.207203  \n10       0.671101      0.245237     0.599881    0.203026  \n11       0.627610      0.275711     0.557799    0.239801  \n12       0.811308      0.008362     0.664787    0.005823  \n13       0.811321      0.007279     0.666611    0.007171  \n14       0.800896      0.016670     0.662219    0.011439  \n15       0.751580      0.086754     0.630598    0.055812  \n16       0.679587      0.163761     0.595991    0.085422  \n17       0.586325      0.256690     0.504586    0.219380  \n18       0.617256      0.249467     0.527081    0.210461  \n19       0.639975      0.241013     0.545022    0.202570  \n20       0.657006      0.232313     0.560969    0.196379  \n21       0.653655      0.220674     0.566869    0.187156  \n22       0.630522      0.222801     0.559650    0.179962  \n23       0.588664      0.254583     0.519160    0.218694  \n24       0.826631      0.009277     0.714694    0.008890  \n25       0.826210      0.006931     0.713868    0.007480  \n26       0.818665      0.012422     0.707843    0.012323  \n27       0.770146      0.084950     0.680688    0.048409  \n28       0.697595      0.164113     0.638587    0.095062  \n29       0.604692      0.256348     0.546041    0.225549  \n30       0.636881      0.250110     0.569011    0.216289  \n31       0.659031      0.241200     0.583903    0.206184  \n32       0.674229      0.231463     0.594184    0.196587  \n33       0.667585      0.220603     0.590174    0.186964  \n34       0.637320      0.231119     0.571981    0.187379  \n35       0.596121      0.260191     0.531046    0.225240  \n36       0.805536      0.010144     0.668982    0.021197  \n37       0.807827      0.010274     0.675478    0.018249  \n38       0.802527      0.011716     0.673000    0.016211  \n39       0.753948      0.085678     0.642936    0.054567  \n40       0.669750      0.185568     0.593853    0.109781  \n41       0.578482      0.265252     0.502270    0.228655  \n42       0.613764      0.260370     0.528564    0.221316  \n43       0.637829      0.251763     0.548161    0.213433  \n44       0.655054      0.242347     0.563376    0.205875  \n45       0.654885      0.229950     0.571124    0.196760  \n46       0.629266      0.233965     0.561356    0.190386  \n47       0.588282      0.262187     0.522276    0.224432  \n48       0.838400      0.006527     0.720647    0.007492  \n49       0.836791      0.006557     0.721526    0.008012  \n50       0.827552      0.015227     0.720188    0.009959  \n51       0.786153      0.073349     0.701487    0.033779  \n52       0.708343      0.168921     0.659317    0.089739  \n53       0.612546      0.264160     0.570088    0.217341  \n54       0.643240      0.255887     0.588873    0.206459  \n55       0.664217      0.245756     0.602901    0.196729  \n56       0.678644      0.235279     0.612221    0.187346  \n57       0.669173      0.225175     0.606719    0.178565  \n58       0.640269      0.233360     0.590158    0.178144  \n59       0.598251      0.263349     0.550737    0.215149  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>cfg</th>\n      <th>fold</th>\n      <th>loss_test</th>\n      <th>acc_test</th>\n      <th>f1_test</th>\n      <th>mean_loss</th>\n      <th>std_loss</th>\n      <th>mean_acc_val</th>\n      <th>std_acc_val</th>\n      <th>mean_acc_train</th>\n      <th>std_acc_train</th>\n      <th>mean_f1_train</th>\n      <th>std_f1_train</th>\n      <th>mean_f1_val</th>\n      <th>std_f1_val</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>18.940403</td>\n      <td>13.612167</td>\n      <td>0.148006</td>\n      <td>0.289398</td>\n      <td>0.013900</td>\n      <td>75.375842</td>\n      <td>0.475101</td>\n      <td>88.254775</td>\n      <td>0.562407</td>\n      <td>0.881785</td>\n      <td>0.005654</td>\n      <td>0.754430</td>\n      <td>0.003962</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13.517536</td>\n      <td>22.813688</td>\n      <td>0.216087</td>\n      <td>0.296447</td>\n      <td>0.022876</td>\n      <td>75.671540</td>\n      <td>1.257495</td>\n      <td>87.909500</td>\n      <td>0.968884</td>\n      <td>0.878310</td>\n      <td>0.009760</td>\n      <td>0.757455</td>\n      <td>0.012299</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>18.105448</td>\n      <td>12.433460</td>\n      <td>0.142572</td>\n      <td>0.301010</td>\n      <td>0.020293</td>\n      <td>75.823471</td>\n      <td>1.143876</td>\n      <td>87.699934</td>\n      <td>0.865473</td>\n      <td>0.876271</td>\n      <td>0.008692</td>\n      <td>0.758931</td>\n      <td>0.011460</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.751302</td>\n      <td>67.984791</td>\n      <td>0.684068</td>\n      <td>0.410880</td>\n      <td>0.191540</td>\n      <td>72.575773</td>\n      <td>5.793029</td>\n      <td>83.027949</td>\n      <td>8.142960</td>\n      <td>0.828278</td>\n      <td>0.083659</td>\n      <td>0.727154</td>\n      <td>0.056903</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1.168189</td>\n      <td>49.581749</td>\n      <td>0.505397</td>\n      <td>0.611775</td>\n      <td>0.437051</td>\n      <td>66.671148</td>\n      <td>12.934831</td>\n      <td>74.474675</td>\n      <td>18.603783</td>\n      <td>0.739389</td>\n      <td>0.193005</td>\n      <td>0.669061</td>\n      <td>0.127240</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2.254341</td>\n      <td>18.631179</td>\n      <td>0.154145</td>\n      <td>0.886728</td>\n      <td>0.732925</td>\n      <td>57.278686</td>\n      <td>24.252981</td>\n      <td>65.782976</td>\n      <td>25.818213</td>\n      <td>0.640302</td>\n      <td>0.283215</td>\n      <td>0.570275</td>\n      <td>0.250451</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>6</td>\n      <td>1</td>\n      <td>12.516399</td>\n      <td>19.201521</td>\n      <td>0.186588</td>\n      <td>0.806154</td>\n      <td>0.706682</td>\n      <td>59.659431</td>\n      <td>23.200582</td>\n      <td>68.741819</td>\n      <td>24.977831</td>\n      <td>0.672284</td>\n      <td>0.273661</td>\n      <td>0.594345</td>\n      <td>0.239270</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>7</td>\n      <td>1</td>\n      <td>9.610622</td>\n      <td>20.152091</td>\n      <td>0.193252</td>\n      <td>0.750074</td>\n      <td>0.677493</td>\n      <td>61.345407</td>\n      <td>22.158502</td>\n      <td>70.762288</td>\n      <td>23.968632</td>\n      <td>0.694280</td>\n      <td>0.262520</td>\n      <td>0.611319</td>\n      <td>0.228306</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>8</td>\n      <td>1</td>\n      <td>8.661695</td>\n      <td>25.057034</td>\n      <td>0.219253</td>\n      <td>0.712597</td>\n      <td>0.647498</td>\n      <td>62.370921</td>\n      <td>21.092658</td>\n      <td>72.070202</td>\n      <td>22.899502</td>\n      <td>0.708747</td>\n      <td>0.250875</td>\n      <td>0.621693</td>\n      <td>0.217251</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>9</td>\n      <td>1</td>\n      <td>2.770374</td>\n      <td>25.893536</td>\n      <td>0.240637</td>\n      <td>0.730237</td>\n      <td>0.616622</td>\n      <td>61.641182</td>\n      <td>20.130258</td>\n      <td>71.050573</td>\n      <td>21.942100</td>\n      <td>0.699167</td>\n      <td>0.239762</td>\n      <td>0.614613</td>\n      <td>0.207203</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>10</td>\n      <td>1</td>\n      <td>2.178082</td>\n      <td>28.022814</td>\n      <td>0.224635</td>\n      <td>0.788966</td>\n      <td>0.616590</td>\n      <td>60.105156</td>\n      <td>19.802989</td>\n      <td>68.309591</td>\n      <td>22.646509</td>\n      <td>0.671101</td>\n      <td>0.245237</td>\n      <td>0.599881</td>\n      <td>0.203026</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>11</td>\n      <td>1</td>\n      <td>2.289579</td>\n      <td>19.125475</td>\n      <td>0.148035</td>\n      <td>0.912474</td>\n      <td>0.718543</td>\n      <td>56.219784</td>\n      <td>22.985344</td>\n      <td>64.405776</td>\n      <td>25.264968</td>\n      <td>0.627610</td>\n      <td>0.275711</td>\n      <td>0.557799</td>\n      <td>0.239801</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>0</td>\n      <td>2</td>\n      <td>5.828178</td>\n      <td>22.509506</td>\n      <td>0.191131</td>\n      <td>0.440478</td>\n      <td>0.017572</td>\n      <td>66.565125</td>\n      <td>0.532008</td>\n      <td>81.254468</td>\n      <td>0.825879</td>\n      <td>0.811308</td>\n      <td>0.008362</td>\n      <td>0.664787</td>\n      <td>0.005823</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5.978181</td>\n      <td>19.695817</td>\n      <td>0.150226</td>\n      <td>0.440467</td>\n      <td>0.014978</td>\n      <td>66.798837</td>\n      <td>0.596181</td>\n      <td>81.240863</td>\n      <td>0.712141</td>\n      <td>0.811321</td>\n      <td>0.007279</td>\n      <td>0.666611</td>\n      <td>0.007171</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3.269951</td>\n      <td>30.684411</td>\n      <td>0.281673</td>\n      <td>0.463411</td>\n      <td>0.036441</td>\n      <td>66.380344</td>\n      <td>1.100915</td>\n      <td>80.200721</td>\n      <td>1.653451</td>\n      <td>0.800896</td>\n      <td>0.016670</td>\n      <td>0.662219</td>\n      <td>0.011439</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0.945817</td>\n      <td>58.821293</td>\n      <td>0.588730</td>\n      <td>0.576282</td>\n      <td>0.198349</td>\n      <td>63.183201</td>\n      <td>5.635736</td>\n      <td>75.401871</td>\n      <td>8.446124</td>\n      <td>0.751580</td>\n      <td>0.086754</td>\n      <td>0.630598</td>\n      <td>0.055812</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1.097103</td>\n      <td>53.117871</td>\n      <td>0.538862</td>\n      <td>0.734099</td>\n      <td>0.362491</td>\n      <td>59.671546</td>\n      <td>8.654326</td>\n      <td>68.577224</td>\n      <td>15.622986</td>\n      <td>0.679587</td>\n      <td>0.163761</td>\n      <td>0.595991</td>\n      <td>0.085422</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2.280688</td>\n      <td>19.809886</td>\n      <td>0.171600</td>\n      <td>0.987963</td>\n      <td>0.657070</td>\n      <td>50.940175</td>\n      <td>21.130167</td>\n      <td>60.606331</td>\n      <td>22.831350</td>\n      <td>0.586325</td>\n      <td>0.256690</td>\n      <td>0.504586</td>\n      <td>0.219380</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>6</td>\n      <td>2</td>\n      <td>4.381033</td>\n      <td>23.840304</td>\n      <td>0.177178</td>\n      <td>0.911900</td>\n      <td>0.636284</td>\n      <td>53.140174</td>\n      <td>20.292666</td>\n      <td>63.435018</td>\n      <td>22.248174</td>\n      <td>0.617256</td>\n      <td>0.249467</td>\n      <td>0.527081</td>\n      <td>0.210461</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>7</td>\n      <td>2</td>\n      <td>5.283458</td>\n      <td>22.547529</td>\n      <td>0.157092</td>\n      <td>0.856075</td>\n      <td>0.613310</td>\n      <td>54.892082</td>\n      <td>19.546129</td>\n      <td>65.507924</td>\n      <td>21.526086</td>\n      <td>0.639975</td>\n      <td>0.241013</td>\n      <td>0.545022</td>\n      <td>0.202570</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>8</td>\n      <td>2</td>\n      <td>3.831899</td>\n      <td>32.091255</td>\n      <td>0.258347</td>\n      <td>0.813936</td>\n      <td>0.590458</td>\n      <td>56.451260</td>\n      <td>18.963056</td>\n      <td>67.051285</td>\n      <td>20.762889</td>\n      <td>0.657006</td>\n      <td>0.232313</td>\n      <td>0.560969</td>\n      <td>0.196379</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>9</td>\n      <td>2</td>\n      <td>3.871399</td>\n      <td>23.536122</td>\n      <td>0.176768</td>\n      <td>0.819556</td>\n      <td>0.560516</td>\n      <td>56.974183</td>\n      <td>18.059832</td>\n      <td>66.650866</td>\n      <td>19.739054</td>\n      <td>0.653655</td>\n      <td>0.220674</td>\n      <td>0.566869</td>\n      <td>0.187156</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>10</td>\n      <td>2</td>\n      <td>1.055093</td>\n      <td>56.425856</td>\n      <td>0.571810</td>\n      <td>0.867215</td>\n      <td>0.555382</td>\n      <td>56.200553</td>\n      <td>17.400038</td>\n      <td>64.408098</td>\n      <td>20.115798</td>\n      <td>0.630522</td>\n      <td>0.222801</td>\n      <td>0.559650</td>\n      <td>0.179962</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>11</td>\n      <td>2</td>\n      <td>2.296078</td>\n      <td>16.197719</td>\n      <td>0.164221</td>\n      <td>0.982693</td>\n      <td>0.655311</td>\n      <td>52.369068</td>\n      <td>20.976534</td>\n      <td>60.876323</td>\n      <td>22.543751</td>\n      <td>0.588664</td>\n      <td>0.254583</td>\n      <td>0.519160</td>\n      <td>0.218694</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.498692</td>\n      <td>79.193610</td>\n      <td>0.791973</td>\n      <td>0.402095</td>\n      <td>0.020770</td>\n      <td>71.527192</td>\n      <td>0.844426</td>\n      <td>82.760444</td>\n      <td>0.924812</td>\n      <td>0.826631</td>\n      <td>0.009277</td>\n      <td>0.714694</td>\n      <td>0.008890</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>1</td>\n      <td>3</td>\n      <td>7.262911</td>\n      <td>22.936478</td>\n      <td>0.176534</td>\n      <td>0.402557</td>\n      <td>0.015134</td>\n      <td>71.453508</td>\n      <td>0.706731</td>\n      <td>82.724424</td>\n      <td>0.688173</td>\n      <td>0.826210</td>\n      <td>0.006931</td>\n      <td>0.713868</td>\n      <td>0.007480</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.579616</td>\n      <td>74.971472</td>\n      <td>0.750021</td>\n      <td>0.420268</td>\n      <td>0.028524</td>\n      <td>70.831526</td>\n      <td>1.255325</td>\n      <td>81.968118</td>\n      <td>1.241464</td>\n      <td>0.818665</td>\n      <td>0.012422</td>\n      <td>0.707843</td>\n      <td>0.012323</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.780494</td>\n      <td>65.994675</td>\n      <td>0.662866</td>\n      <td>0.528486</td>\n      <td>0.189604</td>\n      <td>68.102729</td>\n      <td>4.863261</td>\n      <td>77.220625</td>\n      <td>8.315313</td>\n      <td>0.770146</td>\n      <td>0.084950</td>\n      <td>0.680688</td>\n      <td>0.048409</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2.030025</td>\n      <td>28.527957</td>\n      <td>0.255888</td>\n      <td>0.687300</td>\n      <td>0.360728</td>\n      <td>63.893221</td>\n      <td>9.516253</td>\n      <td>70.354955</td>\n      <td>15.642839</td>\n      <td>0.697595</td>\n      <td>0.164113</td>\n      <td>0.638587</td>\n      <td>0.095062</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>5</td>\n      <td>3</td>\n      <td>2.291271</td>\n      <td>12.248003</td>\n      <td>0.107747</td>\n      <td>0.950278</td>\n      <td>0.673963</td>\n      <td>55.585708</td>\n      <td>20.673400</td>\n      <td>62.361073</td>\n      <td>22.888275</td>\n      <td>0.604692</td>\n      <td>0.256348</td>\n      <td>0.546041</td>\n      <td>0.225549</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>30</td>\n      <td>6</td>\n      <td>3</td>\n      <td>5.458804</td>\n      <td>22.898440</td>\n      <td>0.178341</td>\n      <td>0.871154</td>\n      <td>0.653417</td>\n      <td>57.757004</td>\n      <td>19.867764</td>\n      <td>65.323740</td>\n      <td>22.401107</td>\n      <td>0.636881</td>\n      <td>0.250110</td>\n      <td>0.569011</td>\n      <td>0.216289</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>31</td>\n      <td>7</td>\n      <td>3</td>\n      <td>3.692444</td>\n      <td>32.027387</td>\n      <td>0.277362</td>\n      <td>0.816153</td>\n      <td>0.628325</td>\n      <td>59.153993</td>\n      <td>18.955033</td>\n      <td>67.345984</td>\n      <td>21.628257</td>\n      <td>0.659031</td>\n      <td>0.241200</td>\n      <td>0.583903</td>\n      <td>0.206184</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>8</td>\n      <td>3</td>\n      <td>4.457298</td>\n      <td>27.577025</td>\n      <td>0.209407</td>\n      <td>0.778215</td>\n      <td>0.602080</td>\n      <td>60.107457</td>\n      <td>18.077431</td>\n      <td>68.718008</td>\n      <td>20.760638</td>\n      <td>0.674229</td>\n      <td>0.231463</td>\n      <td>0.594184</td>\n      <td>0.196587</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>33</td>\n      <td>9</td>\n      <td>3</td>\n      <td>2.464701</td>\n      <td>30.315709</td>\n      <td>0.264350</td>\n      <td>0.790917</td>\n      <td>0.572708</td>\n      <td>59.624082</td>\n      <td>17.219157</td>\n      <td>67.984134</td>\n      <td>19.829306</td>\n      <td>0.667585</td>\n      <td>0.220603</td>\n      <td>0.590174</td>\n      <td>0.186964</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>34</td>\n      <td>10</td>\n      <td>3</td>\n      <td>1.924381</td>\n      <td>30.962343</td>\n      <td>0.267599</td>\n      <td>0.855967</td>\n      <td>0.583564</td>\n      <td>57.694033</td>\n      <td>17.522440</td>\n      <td>65.053532</td>\n      <td>21.057903</td>\n      <td>0.637320</td>\n      <td>0.231119</td>\n      <td>0.571981</td>\n      <td>0.187379</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>35</td>\n      <td>11</td>\n      <td>3</td>\n      <td>2.296966</td>\n      <td>18.372005</td>\n      <td>0.117269</td>\n      <td>0.974238</td>\n      <td>0.682670</td>\n      <td>53.926683</td>\n      <td>20.937119</td>\n      <td>61.466154</td>\n      <td>23.418681</td>\n      <td>0.596121</td>\n      <td>0.260191</td>\n      <td>0.531046</td>\n      <td>0.225240</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>36</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7.403112</td>\n      <td>16.964625</td>\n      <td>0.119128</td>\n      <td>0.454879</td>\n      <td>0.023495</td>\n      <td>67.117157</td>\n      <td>2.180876</td>\n      <td>80.671241</td>\n      <td>1.000569</td>\n      <td>0.805536</td>\n      <td>0.010144</td>\n      <td>0.668982</td>\n      <td>0.021197</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>37</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4.995937</td>\n      <td>24.724230</td>\n      <td>0.219541</td>\n      <td>0.449225</td>\n      <td>0.023501</td>\n      <td>67.725341</td>\n      <td>1.850006</td>\n      <td>80.889580</td>\n      <td>1.020508</td>\n      <td>0.807827</td>\n      <td>0.010274</td>\n      <td>0.675478</td>\n      <td>0.018249</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>38</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0.658782</td>\n      <td>72.917459</td>\n      <td>0.728823</td>\n      <td>0.462066</td>\n      <td>0.027778</td>\n      <td>67.479633</td>\n      <td>1.639547</td>\n      <td>80.359702</td>\n      <td>1.166481</td>\n      <td>0.802527</td>\n      <td>0.011716</td>\n      <td>0.673000</td>\n      <td>0.016211</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>39</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3.914467</td>\n      <td>19.969570</td>\n      <td>0.155608</td>\n      <td>0.572289</td>\n      <td>0.194547</td>\n      <td>64.378957</td>\n      <td>5.626853</td>\n      <td>75.627289</td>\n      <td>8.348169</td>\n      <td>0.753948</td>\n      <td>0.085678</td>\n      <td>0.642936</td>\n      <td>0.054567</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>40</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1.882785</td>\n      <td>27.615063</td>\n      <td>0.224291</td>\n      <td>0.756923</td>\n      <td>0.409478</td>\n      <td>59.305896</td>\n      <td>11.341373</td>\n      <td>67.597410</td>\n      <td>17.773932</td>\n      <td>0.669750</td>\n      <td>0.185568</td>\n      <td>0.593853</td>\n      <td>0.109781</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>41</td>\n      <td>5</td>\n      <td>4</td>\n      <td>2.257028</td>\n      <td>15.252948</td>\n      <td>0.135088</td>\n      <td>1.007771</td>\n      <td>0.674059</td>\n      <td>50.742770</td>\n      <td>21.892341</td>\n      <td>59.901167</td>\n      <td>23.655655</td>\n      <td>0.578482</td>\n      <td>0.265252</td>\n      <td>0.502270</td>\n      <td>0.228655</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>42</td>\n      <td>6</td>\n      <td>4</td>\n      <td>7.386781</td>\n      <td>17.991632</td>\n      <td>0.150818</td>\n      <td>0.921627</td>\n      <td>0.658823</td>\n      <td>53.315866</td>\n      <td>21.228828</td>\n      <td>63.148800</td>\n      <td>23.304230</td>\n      <td>0.613764</td>\n      <td>0.260370</td>\n      <td>0.528564</td>\n      <td>0.221316</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>43</td>\n      <td>7</td>\n      <td>4</td>\n      <td>6.499929</td>\n      <td>16.774439</td>\n      <td>0.146436</td>\n      <td>0.861984</td>\n      <td>0.636203</td>\n      <td>55.231389</td>\n      <td>20.495852</td>\n      <td>65.346234</td>\n      <td>22.563879</td>\n      <td>0.637829</td>\n      <td>0.251763</td>\n      <td>0.548161</td>\n      <td>0.213433</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>44</td>\n      <td>8</td>\n      <td>4</td>\n      <td>0.564435</td>\n      <td>75.275770</td>\n      <td>0.752554</td>\n      <td>0.818962</td>\n      <td>0.612088</td>\n      <td>56.703336</td>\n      <td>19.776725</td>\n      <td>66.907616</td>\n      <td>21.730683</td>\n      <td>0.655054</td>\n      <td>0.242347</td>\n      <td>0.563376</td>\n      <td>0.205875</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>45</td>\n      <td>9</td>\n      <td>4</td>\n      <td>4.230159</td>\n      <td>24.800304</td>\n      <td>0.194789</td>\n      <td>0.816713</td>\n      <td>0.580806</td>\n      <td>57.408929</td>\n      <td>18.887688</td>\n      <td>66.807388</td>\n      <td>20.621745</td>\n      <td>0.654885</td>\n      <td>0.229950</td>\n      <td>0.571124</td>\n      <td>0.196760</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>46</td>\n      <td>10</td>\n      <td>4</td>\n      <td>2.133507</td>\n      <td>32.293648</td>\n      <td>0.255840</td>\n      <td>0.869075</td>\n      <td>0.578446</td>\n      <td>56.383943</td>\n      <td>18.330126</td>\n      <td>64.336658</td>\n      <td>21.176163</td>\n      <td>0.629266</td>\n      <td>0.233965</td>\n      <td>0.561356</td>\n      <td>0.190386</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>47</td>\n      <td>11</td>\n      <td>4</td>\n      <td>2.289704</td>\n      <td>22.365919</td>\n      <td>0.122689</td>\n      <td>0.984491</td>\n      <td>0.673236</td>\n      <td>52.656545</td>\n      <td>21.565106</td>\n      <td>60.811820</td>\n      <td>23.408145</td>\n      <td>0.588282</td>\n      <td>0.262187</td>\n      <td>0.522276</td>\n      <td>0.224432</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>48</td>\n      <td>0</td>\n      <td>5</td>\n      <td>7.434802</td>\n      <td>24.381894</td>\n      <td>0.176671</td>\n      <td>0.379040</td>\n      <td>0.012830</td>\n      <td>72.055484</td>\n      <td>0.784554</td>\n      <td>83.932587</td>\n      <td>0.649882</td>\n      <td>0.838400</td>\n      <td>0.006527</td>\n      <td>0.720647</td>\n      <td>0.007492</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>49</td>\n      <td>1</td>\n      <td>5</td>\n      <td>0.514022</td>\n      <td>78.965386</td>\n      <td>0.789361</td>\n      <td>0.383768</td>\n      <td>0.013711</td>\n      <td>72.149370</td>\n      <td>0.810250</td>\n      <td>83.764348</td>\n      <td>0.655411</td>\n      <td>0.836791</td>\n      <td>0.006557</td>\n      <td>0.721526</td>\n      <td>0.008012</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>50</td>\n      <td>2</td>\n      <td>5</td>\n      <td>7.100224</td>\n      <td>13.427159</td>\n      <td>0.097930</td>\n      <td>0.404577</td>\n      <td>0.033809</td>\n      <td>72.014375</td>\n      <td>0.990010</td>\n      <td>82.838593</td>\n      <td>1.522677</td>\n      <td>0.827552</td>\n      <td>0.015227</td>\n      <td>0.720188</td>\n      <td>0.009959</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>51</td>\n      <td>3</td>\n      <td>5</td>\n      <td>3.982442</td>\n      <td>17.839483</td>\n      <td>0.140550</td>\n      <td>0.497902</td>\n      <td>0.164971</td>\n      <td>70.135079</td>\n      <td>3.399416</td>\n      <td>78.781306</td>\n      <td>7.195614</td>\n      <td>0.786153</td>\n      <td>0.073349</td>\n      <td>0.701487</td>\n      <td>0.033779</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>52</td>\n      <td>4</td>\n      <td>5</td>\n      <td>3.074195</td>\n      <td>17.230886</td>\n      <td>0.109658</td>\n      <td>0.667668</td>\n      <td>0.370346</td>\n      <td>65.889755</td>\n      <td>9.035906</td>\n      <td>71.387758</td>\n      <td>16.131524</td>\n      <td>0.708343</td>\n      <td>0.168921</td>\n      <td>0.659317</td>\n      <td>0.089739</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>53</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2.298211</td>\n      <td>5.325219</td>\n      <td>0.038804</td>\n      <td>0.935022</td>\n      <td>0.686798</td>\n      <td>57.961551</td>\n      <td>20.015809</td>\n      <td>62.994077</td>\n      <td>23.858967</td>\n      <td>0.612546</td>\n      <td>0.264160</td>\n      <td>0.570088</td>\n      <td>0.217341</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>54</td>\n      <td>6</td>\n      <td>5</td>\n      <td>4.851910</td>\n      <td>21.681248</td>\n      <td>0.157890</td>\n      <td>0.859018</td>\n      <td>0.662606</td>\n      <td>59.723316</td>\n      <td>19.031802</td>\n      <td>65.827806</td>\n      <td>23.156954</td>\n      <td>0.643240</td>\n      <td>0.255887</td>\n      <td>0.588873</td>\n      <td>0.206459</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>55</td>\n      <td>7</td>\n      <td>5</td>\n      <td>6.053369</td>\n      <td>12.818562</td>\n      <td>0.105500</td>\n      <td>0.806396</td>\n      <td>0.635335</td>\n      <td>61.026003</td>\n      <td>18.140484</td>\n      <td>67.747236</td>\n      <td>22.253673</td>\n      <td>0.664217</td>\n      <td>0.245756</td>\n      <td>0.602901</td>\n      <td>0.196729</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>56</td>\n      <td>8</td>\n      <td>5</td>\n      <td>7.487917</td>\n      <td>11.639407</td>\n      <td>0.079087</td>\n      <td>0.769866</td>\n      <td>0.607868</td>\n      <td>61.886208</td>\n      <td>17.275525</td>\n      <td>69.050499</td>\n      <td>21.303726</td>\n      <td>0.678644</td>\n      <td>0.235279</td>\n      <td>0.612221</td>\n      <td>0.187346</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>57</td>\n      <td>9</td>\n      <td>5</td>\n      <td>2.688849</td>\n      <td>29.478889</td>\n      <td>0.252266</td>\n      <td>0.787365</td>\n      <td>0.579229</td>\n      <td>61.261665</td>\n      <td>16.501380</td>\n      <td>68.035006</td>\n      <td>20.456959</td>\n      <td>0.669173</td>\n      <td>0.225175</td>\n      <td>0.606719</td>\n      <td>0.178565</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>58</td>\n      <td>10</td>\n      <td>5</td>\n      <td>1.193308</td>\n      <td>49.258273</td>\n      <td>0.504290</td>\n      <td>0.848962</td>\n      <td>0.585684</td>\n      <td>59.482351</td>\n      <td>16.710802</td>\n      <td>65.248626</td>\n      <td>21.403909</td>\n      <td>0.640269</td>\n      <td>0.233360</td>\n      <td>0.590158</td>\n      <td>0.178144</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>59</td>\n      <td>11</td>\n      <td>5</td>\n      <td>2.291784</td>\n      <td>27.729175</td>\n      <td>0.160606</td>\n      <td>0.967607</td>\n      <td>0.685043</td>\n      <td>55.896389</td>\n      <td>19.993290</td>\n      <td>61.596576</td>\n      <td>23.805249</td>\n      <td>0.598251</td>\n      <td>0.263349</td>\n      <td>0.550737</td>\n      <td>0.215149</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df=pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  batch_norm  batch_size   f1_mean  f1_confidence  loss_mean  \\\n",
      "0      0        True          16  0.285382       0.186657   8.021037   \n",
      "0      1        True          32  0.310350       0.176673   6.453717   \n",
      "0      2        True          64  0.400204       0.208004   5.942804   \n",
      "0      3        True         512  0.446364       0.180163   2.074904   \n",
      "0      4        True        2048  0.326819       0.122531   1.850459   \n",
      "0      5        True       16384  0.121477       0.034068   2.276308   \n",
      "0      6       False          16  0.170163       0.009899   6.918986   \n",
      "0      7       False          32  0.175928       0.042480   6.227965   \n",
      "0      8       False          64  0.303729       0.170434   5.000649   \n",
      "0      9       False         512  0.225762       0.024920   3.205096   \n",
      "0     10       False        2048  0.364835       0.105409   1.696874   \n",
      "0     11       False       16384  0.142564       0.014143   2.292822   \n",
      "\n",
      "   loss_confidence   acc_mean  acc_confidence  \n",
      "0         4.417082  31.332360       17.776535  \n",
      "0         3.080120  33.827120       16.595378  \n",
      "0         4.788107  40.886792       20.366241  \n",
      "0         1.123128  46.121962       16.459023  \n",
      "0         0.525195  35.214705       10.124634  \n",
      "0         0.013026  14.253447        3.807805  \n",
      "0         2.185475  21.122629        1.618377  \n",
      "0         1.424735  20.864002        4.746556  \n",
      "0         2.098456  34.328098       15.825861  \n",
      "0         0.518447  26.804912        1.940749  \n",
      "0         0.350121  39.392587        8.286138  \n",
      "0         0.002298  20.758058        2.939893  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in mlp_batch['cfg'].unique():\n",
    "    config=get_best_configuration_mlp(idx, param_layers_batch, param_grid_mlp_batch)\n",
    "    batch_norm=config[5]\n",
    "    batch_size=config[9]\n",
    "    f1_dict=mu_confidence_interval(mlp_batch[mlp_batch['cfg'] == idx]['f1_test'])\n",
    "    loss_dict=mu_confidence_interval(mlp_batch[mlp_batch['cfg'] == idx]['loss_test'])\n",
    "    acc_dict=mu_confidence_interval(mlp_batch[mlp_batch['cfg'] == idx]['acc_test'])\n",
    "\n",
    "    new_sample=pd.DataFrame({\n",
    "                'index':[idx],\n",
    "                'batch_norm':[batch_norm],\n",
    "                'batch_size':[batch_size],\n",
    "                'f1_mean': [f1_dict['mu']],\n",
    "                'f1_confidence': [f1_dict['t_student']],\n",
    "                'loss_mean': [loss_dict['mu']],\n",
    "                'loss_confidence': [loss_dict['t_student']],\n",
    "                'acc_mean': [acc_dict['mu']],\n",
    "                'acc_confidence':[acc_dict['t_student']]\n",
    "\n",
    "                })\n",
    "    df=pd.concat([df,new_sample])\n",
    "print(f\"{df}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "res_mlp = find_best_conf(best_cfg, mlp_all)\n",
    "print_statistics_model(res_mlp, \"MLP\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### mlp with different batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "mlp_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scikit learn best cfg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_statistics_sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m res_random_forest \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_statistics_sklearn\u001B[49m(tree_res, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrandom_forest_classifier\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m print_statistics_model(res_random_forest, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrandom forest classifier\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'calculate_statistics_sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_statistics_sklearn(df: pd.DataFrame, model: str) -> Dict:\n",
    "    res = {'f1': mu_confidence_interval(df[df['model'] == model]['f1_test']),\n",
    "           'loss': mu_confidence_interval(df[df['model'] == model]['loss_test']),\n",
    "           'acc': mu_confidence_interval(df[df['model'] == model]['acc_test']),\n",
    "           'conf': df[df['model'] == model]['cfg'].unique()}\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tree based"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_random_forest = calculate_statistics_sklearn(tree_res, 'random_forest_classifier')\n",
    "print_statistics_model(res_random_forest, 'random forest classifier')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decision tree"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_decision_tree = calculate_statistics_sklearn(tree_res, 'decision_tree_classifier')\n",
    "print_statistics_model(res_decision_tree, 'decision tree classifier')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gaussian naive bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_gaussian_nb = calculate_statistics_sklearn(naive_res, 'gaussian_nb')\n",
    "print_statistics_model(res_gaussian_nb, 'gaussianNB')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### QDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration QDA mean metrics:\n",
      "f1_score: 0.5217235201683785 ±0.006131748988284651\n",
      "loss: 0.4649740608914607 ±0.0063746149205477646\n",
      "acc: 0.5350259391085392 ±0.00637461492054775\n",
      "\n",
      "Best hyperparams configuration:\n",
      "[\"{'reg_param': 0.001, 'tol': 0.0001}\"]\n"
     ]
    }
   ],
   "source": [
    "res_qda = calculate_statistics_sklearn(naive_res, 'qda')\n",
    "print_statistics_model(res_qda, 'QDA')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration SVM mean metrics:\n",
      "f1_score: 0.8286206857647119 ±0.003255026164399803\n",
      "loss: 0.17076133850717423 ±0.0031192905594531074\n",
      "acc: 0.8292386614928257 ±0.0031192905594531065\n",
      "\n",
      "Best hyperparams configuration:\n",
      "[\"{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\"]\n"
     ]
    }
   ],
   "source": [
    "res_svm = calculate_statistics_sklearn(svm_res, 'svc')\n",
    "print_statistics_model(res_svm, 'SVM')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}