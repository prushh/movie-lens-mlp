{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we are going to show the main components of the _Feed Forward Neural Network_.\n",
    "Again, statistics will not be saved, in order to avoid overwriting."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch import nn\n",
    "from torch import utils\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset\n",
    "from torchinfo import summary\n",
    "\n",
    "from src.data.dataset import MovieDataset\n",
    "from src.models.config import best_param_layers, best_param_grid_mlp\n",
    "from src.models.network.mlp import execute\n",
    "from src.models.network.validate import test_eval\n",
    "from src.utils.const import DATA_DIR, SEED, NUM_BINS, NETWORK_RESULTS_DIR\n",
    "from src.utils.util_models import fix_random, balancer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Useful path to data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.join(os.getcwd(), '..')\n",
    "PROCESSED_DIR = os.path.join(ROOT_DIR, DATA_DIR, 'processed')\n",
    "INTERIM_MODEL_FOLDER = os.path.join('..', NETWORK_RESULTS_DIR, 'mlp')\n",
    "if not os.path.exists(INTERIM_MODEL_FOLDER):\n",
    "    os.mkdir(INTERIM_MODEL_FOLDER)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fix random seed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "fix_random(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given PyTorch's support for the use of GPUs, we check which hardware the training will run on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('Using device:', torch.cuda.get_device_name(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import final dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "final_stored = pd.read_parquet(os.path.join(PROCESSED_DIR, 'final.parquet'))\n",
    "final = MovieDataset(final_stored)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The MovieDataset class object represents the dataset that is used within the neural network. This class implements various methods, which are useful for applying data transformations using indices.\n",
    "In addition, the constructor performs the following operations:\n",
    "- creating a dictionary to easily access the indices of a specific feature\n",
    "- split between data and target feature\n",
    "- target feature discretization\n",
    "- conversion of data and target features into tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.idx_column = {}\n",
    "        for idx, col_name in enumerate(df.columns):\n",
    "            self.idx_column[col_name] = idx\n",
    "\n",
    "        X, y_continuous = self.data_target_split(df)\n",
    "\n",
    "        self.num_classes = NUM_BINS\n",
    "        y = self._discretize(y_continuous)\n",
    "\n",
    "        self.X = torch.tensor(X, dtype=torch.float)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple:\n",
    "        return self.X[idx, :], self.y[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def data_target_split(df: pd.DataFrame) -> Tuple:\n",
    "        y = df['rating_mean']\n",
    "        X = df.drop(columns='rating_mean').to_numpy()\n",
    "        return X, y\n",
    "\n",
    "    def _discretize(self, target: pd.Series) -> pd.Series:\n",
    "        y = pd.cut(target, bins=self.num_classes, labels=False)\n",
    "        return y\n",
    "\n",
    "    def scale(self, train_idx, test_idx, scaler, features: List[int]):\n",
    "        train_data = self.X[train_idx]\n",
    "        test_data = self.X[test_idx]\n",
    "\n",
    "        for feature in features:\n",
    "            feature_train = train_data[:, feature].reshape(-1, 1)\n",
    "            feature_test = test_data[:, feature].reshape(-1, 1)\n",
    "\n",
    "            scaled_train = np.squeeze(scaler.fit_transform(feature_train))\n",
    "            scaled_test = np.squeeze(scaler.transform(feature_test))\n",
    "\n",
    "            self.X[train_idx, feature] = torch.tensor(scaled_train, dtype=torch.float)\n",
    "            self.X[test_idx, feature] = torch.tensor(scaled_test, dtype=torch.float)\n",
    "\n",
    "    def normalize(self, train_idx, test_idx, norm: str = 'l2'):\n",
    "        train_data = self.X[train_idx]\n",
    "        test_data = self.X[test_idx]\n",
    "\n",
    "        norm_train = normalize(train_data, norm=norm)\n",
    "        norm_test = normalize(test_data, norm=norm)\n",
    "\n",
    "        self.X[train_idx, :] = torch.tensor(norm_train, dtype=torch.float)\n",
    "        self.X[test_idx, :] = torch.tensor(norm_test, dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The MovieNet class defines at runtime the architecture of our Neural Network following the parameter specification, this was made possible by using ModuleList.\n",
    "It was decided to keep the size of the hidden layer fixed at the nearest power of two defined by $2 \\over 3$ of the number of features.\n",
    "Since we have $1153$ features, ${1153 * 2 \\over 3} = 769$, so the nearest power of two is $512$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MovieNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            input_act: nn.Module,\n",
    "            hidden_size: int,\n",
    "            hidden_act: nn.Module,\n",
    "            num_hidden_layers: int,\n",
    "            output_fn,\n",
    "            num_classes: int,\n",
    "            dropout: float = 0.0,\n",
    "            batch_norm: bool = False\n",
    "    ) -> None:\n",
    "        super(MovieNet, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            input_act\n",
    "        ])\n",
    "\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "            if batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "\n",
    "            self.layers.append(hidden_act)\n",
    "\n",
    "            if dropout > 0.0:\n",
    "                self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "        if output_fn:\n",
    "            self.layers.append(output_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def reset_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train & Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As with the sklearn models, the two cross validations were applied, one to check that the model works well on several test sets and the other to perform the optimization of the hyperparameters that is not performed using _GridSearchCV_ because of compatibility. For this reason _itertools_ has been used to calculate the Cartesian product of the hyperparameters. Scaling and balancing transformations are then applied before loading the data into the respective _DataLoader_.\n",
    "The training phase also includes the following components:\n",
    "- _optimizer_, whose task is to try to minimize the loss function. It has been defined of two types \\[_Adam, SGD_\\], both have weight decay while only the second uses momentum. These two parameters act on the updating of weights within the network to avoid overfitting (weight decay) and improve both training speed and accuracy (momentum).\n",
    "- _scheduler_, which acts on the learning rate value by decreasing it every set number of epochs by a certain gamma value.\n",
    "- _loss function_, only _CrossEntropy_ was used, which allows us to assess how our model is performing. It includes _SoftMax_ activation function and therefore no output layer was added to the network architecture.\n",
    "- _early stopping_, which allows us to stop the training in advance if the loss does not vary for some epoch during validation.\n",
    "\n",
    "In addition, the validate and test functions have also been implemented, both of which have the task of calculating metrics. The test additionally provides the possibility of using the classification_report function and printing the roc plot. All info outputs and plots were implemented via the tensorboard library."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_test(dataset: MovieDataset):\n",
    "    features = [\n",
    "        dataset.idx_column['year'],\n",
    "        dataset.idx_column['title_length'],\n",
    "        dataset.idx_column['tag_count'],\n",
    "        dataset.idx_column['runtime'],\n",
    "        dataset.idx_column['rating_count']\n",
    "    ]\n",
    "    num_workers = 2\n",
    "\n",
    "    n_splits = 5\n",
    "    cv_outer = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(cv_outer.split(dataset.X, y=dataset.y), 1):\n",
    "        hyper_parameters_model = itertools.product(\n",
    "            best_param_layers['input_act'],\n",
    "            best_param_layers['hidden_act'],\n",
    "            best_param_layers['hidden_size'],\n",
    "            best_param_layers['num_hidden_layers'],\n",
    "            best_param_layers['dropout'],\n",
    "            best_param_layers['batch_norm'],\n",
    "            best_param_layers['output_fn'],\n",
    "            best_param_grid_mlp['starting_lr'],\n",
    "            best_param_grid_mlp['num_epochs'],\n",
    "            best_param_grid_mlp['batch_size'],\n",
    "            best_param_grid_mlp['optim'],\n",
    "            best_param_grid_mlp['momentum'],\n",
    "            best_param_grid_mlp['weight_decay'],\n",
    "        )\n",
    "\n",
    "        print('=' * 65)\n",
    "        print(f'Fold {fold}')\n",
    "\n",
    "        list_fold_stat = []\n",
    "\n",
    "        data_test = utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "        loader_test = utils.data.DataLoader(data_test, batch_size=1,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "        for idx, (input_act,\n",
    "                  hidden_act,\n",
    "                  hidden_size,\n",
    "                  num_hidden_layers,\n",
    "                  dropout,\n",
    "                  batch_norm,\n",
    "                  _,\n",
    "                  starting_lr,\n",
    "                  num_epochs,\n",
    "                  batch_size,\n",
    "                  optimizer_class,\n",
    "                  momentum,\n",
    "                  weight_decay) in enumerate(hyper_parameters_model):\n",
    "\n",
    "            best_val_network = None\n",
    "            max_f1_val = 0\n",
    "\n",
    "            cfg = (\n",
    "                input_act, hidden_act, hidden_size, num_hidden_layers, dropout, batch_norm, starting_lr, num_epochs,\n",
    "                batch_size, optimizer_class, momentum, weight_decay)\n",
    "\n",
    "            cv_inner = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "            for inner_fold, (inner_train_idx, val_idx) in enumerate(\n",
    "                    cv_inner.split(dataset.X[train_idx], y=dataset.y[train_idx]), 1):\n",
    "\n",
    "                # Balancing\n",
    "                train_target = dataset.y[inner_train_idx]\n",
    "                sampler = balancer(train_target)\n",
    "\n",
    "                # Scaling\n",
    "                scaler = preprocessing.MinMaxScaler()\n",
    "                dataset.scale(train_idx, test_idx, scaler, features)\n",
    "\n",
    "                data_train = utils.data.Subset(dataset, inner_train_idx)\n",
    "                data_val = utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "                loader_train = utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                                     sampler=sampler,\n",
    "                                                     pin_memory=True,\n",
    "                                                     num_workers=num_workers)\n",
    "\n",
    "                loader_val = utils.data.DataLoader(data_val, batch_size=1,\n",
    "                                                   shuffle=False,\n",
    "                                                   num_workers=num_workers)\n",
    "\n",
    "                input_size = dataset.X.shape[1]\n",
    "                num_classes = dataset.num_classes\n",
    "                network = MovieNet(input_size=input_size,\n",
    "                                   input_act=input_act,\n",
    "                                   hidden_size=hidden_size,\n",
    "                                   hidden_act=hidden_act,\n",
    "                                   num_hidden_layers=num_hidden_layers,\n",
    "                                   dropout=dropout,\n",
    "                                   output_fn=None,\n",
    "                                   num_classes=num_classes)\n",
    "                network.reset_weights()\n",
    "                network.to(device)\n",
    "\n",
    "                if fold == 1 and inner_fold == 1:\n",
    "                    print('=' * 65)\n",
    "                    print(f'Configuration [{idx}]: {cfg}')\n",
    "                    summary(network)\n",
    "\n",
    "                name_train = f'movie_net_experiment_{idx}'\n",
    "\n",
    "                if optimizer_class == torch.optim.Adam:\n",
    "                    optimizer = optimizer_class(network.parameters(),\n",
    "                                                lr=starting_lr,\n",
    "                                                weight_decay=weight_decay)\n",
    "                else:\n",
    "                    optimizer = optimizer_class(network.parameters(),\n",
    "                                                lr=starting_lr,\n",
    "                                                momentum=momentum,\n",
    "                                                weight_decay=weight_decay)\n",
    "\n",
    "                fold_stat = execute(name_train,\n",
    "                                        network,\n",
    "                                        optimizer,\n",
    "                                        num_epochs,\n",
    "                                        loader_train,\n",
    "                                        loader_val,\n",
    "                                        device)\n",
    "                list_fold_stat.append(fold_stat)\n",
    "\n",
    "                if fold_stat['f1_val'] >= max_f1_val:\n",
    "                    max_f1_val = fold_stat['f1_val']\n",
    "                    best_val_network = network\n",
    "\n",
    "            path = os.path.join(INTERIM_MODEL_FOLDER, f'{fold}_network.pt')\n",
    "            torch.save(best_val_network, path)\n",
    "\n",
    "            criterion = CrossEntropyLoss()\n",
    "            loss_test, acc_test, f1_test = test_eval(fold, loader_test, device, criterion, notebook=True)\n",
    "            print(f'Test {fold}, loss={loss_test:3f}, accuracy={acc_test:3f}, f1={f1_test:3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Fold 1\n",
      "=================================================================\n",
      "Configuration [0]: (LeakyReLU(negative_slope=0.01), LeakyReLU(negative_slope=0.01), 512, 3, 0.2, True, 0.001, 50, 128, <class 'torch.optim.adam.Adam'>, 0.9, 1e-05)\n",
      "Epoch: 1  Lr: 0.00100000  Loss: Train = [1.3993] - Val = [0.8860]  Accuracy: Train = [42.73%] - Val = [61.79%]  F1: Train = [0.416] - Val = [0.636]  Time one epoch (s): 9.0810 \n",
      "Epoch: 2  Lr: 0.00100000  Loss: Train = [0.5866] - Val = [0.5850]  Accuracy: Train = [74.58%] - Val = [75.24%]  F1: Train = [0.744] - Val = [0.751]  Time one epoch (s): 9.6167 \n",
      "Epoch: 3  Lr: 0.00100000  Loss: Train = [0.4499] - Val = [0.6101]  Accuracy: Train = [80.85%] - Val = [73.76%]  F1: Train = [0.807] - Val = [0.740]  Time one epoch (s): 9.3401 \n",
      "Epoch: 4  Lr: 0.00100000  Loss: Train = [0.3641] - Val = [0.5127]  Accuracy: Train = [84.70%] - Val = [78.71%]  F1: Train = [0.846] - Val = [0.787]  Time one epoch (s): 9.0880 \n",
      "Epoch: 5  Lr: 0.00100000  Loss: Train = [0.3123] - Val = [0.5086]  Accuracy: Train = [87.08%] - Val = [80.70%]  F1: Train = [0.870] - Val = [0.807]  Time one epoch (s): 9.0100 \n",
      "Epoch: 6  Lr: 0.00010000  Loss: Train = [0.2270] - Val = [0.4657]  Accuracy: Train = [90.69%] - Val = [81.27%]  F1: Train = [0.907] - Val = [0.814]  Time one epoch (s): 9.5294 \n"
     ]
    }
   ],
   "source": [
    "train_test(final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}