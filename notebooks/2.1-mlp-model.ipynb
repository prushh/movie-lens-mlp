{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch import nn\n",
    "from torch import utils\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset\n",
    "from torchinfo import summary\n",
    "\n",
    "from src.data.dataset import MovieDataset\n",
    "from src.models.config import best_param_layers, best_param_grid_mlp\n",
    "from src.models.network.mlp import execute\n",
    "from src.models.network.validate import validate, test_eval\n",
    "from src.utils.const import DATA_DIR, SEED, NUM_BINS\n",
    "from src.utils.util_models import fix_random, balancer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Useful path to data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.join(os.getcwd(), '..')\n",
    "PROCESSED_DIR = os.path.join(ROOT_DIR, DATA_DIR, 'processed')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fix random seed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fix_random(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('Using device:', torch.cuda.get_device_name(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import final dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_stored = pd.read_parquet(os.path.join(PROCESSED_DIR, 'final.parquet'))\n",
    "final = MovieDataset(final_stored)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.idx_column = {}\n",
    "        for idx, col_name in enumerate(df.columns):\n",
    "            self.idx_column[col_name] = idx\n",
    "\n",
    "        X, y_continuous = self.data_target_split(df)\n",
    "\n",
    "        self.num_classes = NUM_BINS\n",
    "        y = self._discretize(y_continuous)\n",
    "\n",
    "        self.X = torch.tensor(X, dtype=torch.float)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple:\n",
    "        return self.X[idx, :], self.y[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def data_target_split(df: pd.DataFrame) -> Tuple:\n",
    "        y = df['rating_mean']\n",
    "        X = df.drop(columns='rating_mean').to_numpy()\n",
    "        return X, y\n",
    "\n",
    "    def _discretize(self, target: pd.Series) -> pd.Series:\n",
    "        y = pd.cut(target, bins=self.num_classes, labels=False)\n",
    "        return y\n",
    "\n",
    "    def scale(self, train_idx, test_idx, scaler, features: List[int]):\n",
    "        train_data = self.X[train_idx]\n",
    "        test_data = self.X[test_idx]\n",
    "\n",
    "        for feature in features:\n",
    "            feature_train = train_data[:, feature].reshape(-1, 1)\n",
    "            feature_test = test_data[:, feature].reshape(-1, 1)\n",
    "\n",
    "            scaled_train = np.squeeze(scaler.fit_transform(feature_train))\n",
    "            scaled_test = np.squeeze(scaler.transform(feature_test))\n",
    "\n",
    "            self.X[train_idx, feature] = torch.tensor(scaled_train, dtype=torch.float)\n",
    "            self.X[test_idx, feature] = torch.tensor(scaled_test, dtype=torch.float)\n",
    "\n",
    "    def normalize(self, train_idx, test_idx, norm: str = 'l2'):\n",
    "        train_data = self.X[train_idx]\n",
    "        test_data = self.X[test_idx]\n",
    "\n",
    "        norm_train = normalize(train_data, norm=norm)\n",
    "        norm_test = normalize(test_data, norm=norm)\n",
    "\n",
    "        self.X[train_idx, :] = torch.tensor(norm_train, dtype=torch.float)\n",
    "        self.X[test_idx, :] = torch.tensor(norm_test, dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MovieNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            input_act: nn.Module,\n",
    "            hidden_size: int,\n",
    "            hidden_act: nn.Module,\n",
    "            num_hidden_layers: int,\n",
    "            output_fn,\n",
    "            num_classes: int,\n",
    "            dropout: float = 0.0,\n",
    "            batch_norm: bool = False\n",
    "    ) -> None:\n",
    "        super(MovieNet, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            input_act\n",
    "        ])\n",
    "\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "            if batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "\n",
    "            self.layers.append(hidden_act)\n",
    "\n",
    "            if dropout > 0.0:\n",
    "                self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "        if output_fn:\n",
    "            self.layers.append(output_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def reset_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train & Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_test(dataset: MovieDataset):\n",
    "    features = [\n",
    "        dataset.idx_column['year'],\n",
    "        dataset.idx_column['title_length'],\n",
    "        dataset.idx_column['tag_count'],\n",
    "        dataset.idx_column['runtime'],\n",
    "        dataset.idx_column['rating_count']\n",
    "    ]\n",
    "    num_workers = 2\n",
    "\n",
    "    n_splits = 5\n",
    "    cv_outer = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(cv_outer.split(dataset.X, y=dataset.y), 1):\n",
    "        hyper_parameters_model_all = itertools.product(\n",
    "            best_param_layers['input_act'],\n",
    "            best_param_layers['hidden_act'],\n",
    "            best_param_layers['hidden_size'],\n",
    "            best_param_layers['num_hidden_layers'],\n",
    "            best_param_layers['dropout'],\n",
    "            best_param_layers['batch_norm'],\n",
    "            best_param_layers['output_fn'],\n",
    "            best_param_grid_mlp['starting_lr'],\n",
    "            best_param_grid_mlp['num_epochs'],\n",
    "            best_param_grid_mlp['batch_size'],\n",
    "            best_param_grid_mlp['optim'],\n",
    "            best_param_grid_mlp['momentum'],\n",
    "            best_param_grid_mlp['weight_decay']\n",
    "        )\n",
    "\n",
    "        hyper_parameters_model = hyper_parameters_model_all\n",
    "\n",
    "        print('=' * 65)\n",
    "        print(f'Fold {fold}')\n",
    "\n",
    "        data_test = utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "        loader_test = utils.data.DataLoader(data_test, batch_size=1,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "        for idx, (input_act,\n",
    "                  hidden_act,\n",
    "                  hidden_size,\n",
    "                  num_hidden_layers,\n",
    "                  dropout,\n",
    "                  batch_norm,\n",
    "                  _,\n",
    "                  starting_lr,\n",
    "                  num_epochs,\n",
    "                  batch_size,\n",
    "                  optimizer_class,\n",
    "                  momentum,\n",
    "                  weight_decay) in enumerate(hyper_parameters_model):\n",
    "\n",
    "            cfg = (\n",
    "                input_act, hidden_act, hidden_size, num_hidden_layers, dropout, batch_norm, starting_lr, num_epochs,\n",
    "                batch_size, optimizer_class, momentum, weight_decay)\n",
    "\n",
    "            cv_inner = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "            for inner_fold, (inner_train_idx, val_idx) in enumerate(\n",
    "                    cv_inner.split(dataset.X[train_idx], y=dataset.y[train_idx]), 1):\n",
    "\n",
    "                # Balancing\n",
    "                train_target = dataset.y[inner_train_idx]\n",
    "                sampler = balancer(train_target)\n",
    "\n",
    "                # Scaling\n",
    "                scaler = preprocessing.MinMaxScaler()\n",
    "                dataset.scale(train_idx, test_idx, scaler, features)\n",
    "\n",
    "                data_train = utils.data.Subset(dataset, inner_train_idx)\n",
    "                data_val = utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "                loader_train = utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                                     sampler=sampler,\n",
    "                                                     pin_memory=True,\n",
    "                                                     num_workers=num_workers)\n",
    "\n",
    "                loader_val = utils.data.DataLoader(data_val, batch_size=1,\n",
    "                                                   shuffle=False,\n",
    "                                                   num_workers=num_workers)\n",
    "\n",
    "                input_size = dataset.X.shape[1]\n",
    "                num_classes = dataset.num_classes\n",
    "                network = MovieNet(input_size=input_size,\n",
    "                                   input_act=input_act,\n",
    "                                   hidden_size=hidden_size,\n",
    "                                   hidden_act=hidden_act,\n",
    "                                   num_hidden_layers=num_hidden_layers,\n",
    "                                   dropout=dropout,\n",
    "                                   output_fn=None,\n",
    "                                   num_classes=num_classes)\n",
    "                network.reset_weights()\n",
    "                network.to(device)\n",
    "\n",
    "                if fold == 1 and inner_fold == 1:\n",
    "                    print('=' * 65)\n",
    "                    print(f'Configuration [{idx}]: {cfg}')\n",
    "                    summary(network)\n",
    "\n",
    "                # TODO: fix experiment name\n",
    "                name_train = f'movie_net_experiment_{idx}'\n",
    "\n",
    "                if optimizer_class == torch.optim.Adam:\n",
    "                    optimizer = optimizer_class(network.parameters(),\n",
    "                                                lr=starting_lr,\n",
    "                                                weight_decay=weight_decay)\n",
    "                else:\n",
    "                    optimizer = optimizer_class(network.parameters(),\n",
    "                                                lr=starting_lr,\n",
    "                                                momentum=momentum,\n",
    "                                                weight_decay=weight_decay)\n",
    "\n",
    "                fold_stat = execute(name_train,\n",
    "                                    network,\n",
    "                                    optimizer,\n",
    "                                    num_epochs,\n",
    "                                    loader_train,\n",
    "                                    loader_val,\n",
    "                                    device)\n",
    "\n",
    "            criterion = CrossEntropyLoss()\n",
    "            loss_test, acc_test, f1_test = test_eval(fold, loader_test, device, criterion, True)\n",
    "            print(f'Test {fold}, loss={loss_test:3f}, accuracy={acc_test:3f}, f1={f1_test:3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_test(final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}