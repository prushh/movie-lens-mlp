\documentclass[../main]{subfiles}

\begin{document}

\chapter{Methodology}
In the next chapter there will be the explaination of the data pipeline that the project followed.
In particular, each subsection will focus on a specific task, except for the data visualization that has been used only when needed.
% TODO: add references to Pandas an Numpy (?)

\section{Data Acquisition}
The used datasets are downloaded at runtime directly from the sources.
These datasets come directly from MovieLens' page and provides 6 different files.
More informations about the nature of the data are available \href{https://files.grouplens.org/datasets/movielens/ml-latest-README.html}{here}.
    \begin{table}[h]
        \center
        \begin{tabular}{|l | l|}
        \hline
        \textbf{Dataset} & \textbf{Features} \\
        \hline
        ratings.csv &  userId, movieId, rating, timestamp\\
        \hline
        tags.csv &  userId, movieId, tag, timestamp\\
        \hline
        movies.csv &  movieId, title, genres\\
        \hline
        links.csv &  movieId, imdbId, tmdbId\\
        \hline
        genome-scores.csv &  movieId, tagId, relevance\\
        \hline
        genome-tags.csv & tagId, tag\\
        \hline
        \end{tabular}
    \end{table}
        


Most of these datasets provides information for approximately 60000 films.
The links dataset provides two identifiers that allow to collect information from the IMDb and TMDB databases.
Thanks to the links' features, it has been possible to collect some more information on the running times of the films that could provide more insight into them.
Talking about the ground truth of the supervisioned models, the rating mean is missing. So the target feature will be computed during the pre-process phase thanks to the ratings dataset.
Further information about the features usage and the cardinality of the datasets will be discussed in the following section.


\section{Data Pre-process}
In this section, will be discussed the pre-process phase for each of the above presented datasets.
In order to achieve a major clearity, the work on each dataset will be discussed in a specific subsection where the operation computed on them will be explained.
At the beginning of each subsection the cardinality will be reported.

\subsection*{Movies - movies.csv}
Cardinality: $58098 \times 3$

Inside this dataset, the title and generes features contains multiple information.
In particular, the title has been splitted in two part, where on one hand there's only the title name, and on the other, there's the year of the film production.
Since the title name is a string, that doesn't add more information to a classic machine learning model, this feature has been converted into its length meanwhile the production year just constitutes a new feature.
The genres feature contains a pipe separated list of a fixed possible values.
Since the list is just saying if a film has a specific genre or not, to each film, all the fixed values has been added as a feature, and if a genre appears into the genres feature, that column will result into 1 that indicate the presence of that genre, 0 otherwise.
At the end of this initial phase, the first sample of the movies dataset appears as shown in the Table \ref{table:movies-first-sample}.
\begin{table}[h]
    \center
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{movieId} & \textbf{title\_len} & \textbf{year} & \textbf{action} & \textbf{adventure} & \textbf{\dots}  & \textbf{Western} & \textbf{(no genres listed)} \\
    \hline
    1                & 16                  & 1995          & 0               & 1                  & \dots           & 0                & 0                           \\
    \hline
    \end{tabular}
    \caption{First sample of movis interim dataset.}
    \label{table:movies-first-sample}
\end{table}

In order to finish the data pre-process on this dataset, the data cleaning is required.
First of all, there are some films where the year of the film production is missing.
Analyzing the distribuition of these, as showed in Figure \ref{fig:year_distribution_missing}, it is possible to see that the distribuition is right skewed, so the missing values can be filled with the median, as suggested during the lectures.
\begin{figure}[h]
    \includegraphics[width=\linewidth]{figures/year_distribution_missing.png}
    \caption{Right skewed probability distribution of the year feature.}
    \label{fig:year_distribution_missing}
\end{figure}

Since the feature (no genres listed) and the films that has no genres provide no information about the film, they will be removed obtaining a final cardinality of $53832 \times 22$.

\subsection*{Tags - tags.csv}
Cardinality: $1108997 \times 4$

In order to use the relevant data from this dataset the timestamp and userId features have been deleted because they don't explain anything more about the films.
The information provided by each sample in the dataset are not very meaningful, for this reason it was decided to count the number of tags associated with each film, in order to understand how much interaction that film generated.
When a film doesn't have any related tag, the tag\_count can be setted to 0 because no users were interested on that film.
After these operations the cardinality was reduced to $45981 \times 2$.

\subsection*{Ratings - ratings.csv}
Cardinality: $27753443 \times 2$

The features in this dataset were necessary to find the target column.
In order to compute it, the average of each films' ratings has been calculated.
In addition, the number of the rating on each film has been counted.
However, the task of this project is the classification, for this reason the rating\_mean has been discretized in 10 bins, where each of them covers a rating range of 0.45 as showed in Figure \ref{fig:rating_mean_discretized}.

\begin{figure}[h]
    \center
    \includegraphics[width=\linewidth]{figures/step_rating_mean_discretized.png}
    \caption{Discretization of the rating\_mean feature.}
    \label{fig:rating_mean_discretized}
\end{figure}

At the end, the final cardinality is $53889 \times 4$.

\subsection*{Genome - genome-scores.csv and genome-tags.csv}
Cardinality scores: $14862528 \times 3$

\noindent
Cardinality tags: $1128 \times 2$

Talking about these two datasets, the merge operation over the tagId was needed because it was interesting to associate the tagId with its string name.
After this union, on each sample there is the correspondence between a movieId, the tag name and the relevance of that tag.
The final step consist on relate every film to the relevance of each tag, using the pivot function, as shown in Table \ref{table:genome-first-sample}.
\begin{table}[h]
    \center
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{movieId} & \textbf{007}     & \textbf{18th century} & \textbf{action}  & \textbf{absurd} & \textbf{\dots} & \textbf{addiction} & \textbf{adventure} \\
    \hline
    1       & 0.02900 & 0.05425      & 0.66825 & 0.09725          & \dots & 0.07475   & 0.90700 \\
    \hline
    \end{tabular}
    \caption{First sample of genome interim dataset.}
    \label{table:genome-first-sample}
\end{table}

The final dataset named genome.csv has the cardinality equal to $13176 \times 1129$.

\subsection*{Links - links.csv}
Cardinality: $58098 \times 3$

On this dataset no pre-processing was needed because for each movie it contains two identifier that provide a link to external sources.
The interesting one is tmdbId because it has been used to retrieve information from the TMDB database.
Using the TMDB Api it has been possibile to build a new dataset that contains some additional features: budget, revenue, adult and runtime.
In order to avoid the expensive operation of the Api calls, the resulted dataset has been saved in a GitHub \href{https://github.com/prushh/movie-lens-mlp/releases/tag/api-dataset}{release}.
Due to the insufficient amount of valid samples, the IMDb \href{https://datasets.imdbws.com/title.basics.tsv.gz}{title-basics.csv} dataset was downloaded to try to fill in all missing values.
After a brief exploration and analysis it has been possible to see that only the runtime feature has an enough amount of samples.
Since there continue to be some missing values, the distribution of this feature was analyzed in Figure \ref{fig:runtime_distribution_missing}.
It is possible to see that the distribuition is left skewed, so the missing values can be filled with the median.

\begin{figure}[h]
    \center
    \includegraphics[width=\linewidth]{figures/runtime_distribution_missing.png}
    \caption{Left skewed probability distribution of the runtime feature.}
    \label{fig:runtime_distribution_missing}
\end{figure}

The final cardinality of the acquired external resources is $58098 \times 2$.

\subsection*{Final - final.parquet}
In this section it will be showed how the cardinality of the used dataset has been obtained, starting from all the previously processed datasets.
In particular, the focus will be on the number of samples, because the number of columns will increase by the addition of the features coming from all the other datasets.
Since for a classification task is interesting to know the informations about a sample, the starting dataset will be the movies and then all the others will be merged, trying to preserve the maximum number of samples.
The first merge has been between movies and ratings because if a film doesnâ€™t have the target feature there is no point to consider it.
In addition, all films with no rating mean were discarded, resulting in a cardinality of $50157 \times 24$.
From this point on, all the merges will be performed using the previously calculated dataset, so only the new dataset will be specified.
The second merge considers tags dataset, where there was an important number of samples that hasn't a tag count.
In this case, these samples were not discarded because they were considered as a film that generated no users interaction, so all missing values were filled with 0, resulting in a cardinality of $50157 \times 25$.
The third merge introduces external resources where for each film there is a related sample, so the cardinality depends from the previous one, resulting $50157 \times 26$.
The final merge introduces the genome dataset, where there is a large gap between the cardinality of this dataset and the previous one.
The choice for this merge was to discard all data that don't match with the genome dataset because it provides more interesting characteristics than the other datasets could provide.
At the end, the cardinality of the final dataset is $13147 \times 1154$.

\subsection*{Data Transformation}
After further exploration, it becomes apparent that some features did not belong to a well-defined range.
For this reason, it was useful to apply certain transformation techniques such as min-max scaling and normalization.
In order to apply some of these transformations, taking into account also the balancing task, the split of the dataset is needed because all these operations entail working on the training set.
So, in this section, the dataset will be split with the ratios shown in Table \ref{table:dataset_split}.
% TODO: change size value!
\begin{table}[h]
    \center
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Train set size} & \textbf{Validation set size} & \textbf{Test set size} \\
        \hline
        72\%                    & 8\%                          & 20\%                   \\
        \hline
    \end{tabular}
    \caption{Train/Validation/Test set ratios.}
    \label{table:dataset_split}
\end{table}

The features that have been analyzed are: title\_length, runtime, rating\_count, tag\_count, year.
It has been seen as the distribuition of these features are not Gaussian and for this reason the min-max scaling has been applied.
\begin{figure}[htb]
    \begin{minipage}[t]{.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/original_title_len.png}
        \includegraphics[width=\linewidth]{figures/minmax_title_len.png}
        \subcaption{Title length}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/original_runtime.png}
        \includegraphics[width=\linewidth]{figures/minmax_runtime.png}
        \subcaption{Runtime}
    \end{minipage}
    \begin{minipage}[t]{.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/original_rating_count.png}
        \includegraphics[width=\linewidth]{figures/minmax_rating_count.png}
        \subcaption{Rating count}
    \end{minipage}  
    \begin{minipage}[t]{.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/original_tag_count.png}
        \includegraphics[width=\linewidth]{figures/minmax_tag_count.png}
        \subcaption{Tag count}
    \end{minipage}  
    \begin{minipage}[t]{.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/original_year.png}
        \includegraphics[width=\linewidth]{figures/minmax_year.png}
        \subcaption{Year}
    \end{minipage}    
    \label{fig:distribuition_features}
    \caption{Comparision of feature distribuitions.}
\end{figure}

In order to have a greater reliability on the techniques to apply, a test function was created.
This function uses the default version of the scikit models used in this project, which are: RandomForest, DecisionTree, GaussianNB, QDA and SVM.
Only the scikit models were used due to their ease of use, which is why the Neural Network does not appear in these tests.
From the outcomes, some graphs were produced to better interpret the results.
Thanks to this function, 3 configuration has tried:
\begin{itemize}
    \item Minmax scaling with normalization of all non categorical features
    \item Minmax scaling
    \item Normalization of all non categorical features
\end{itemize}

After the tests it turned out that the normalization technique in addition to minmax scaling did not provide any improvement, as shown in Figure \ref{fig:plot_data_transformation}.
In the last configuration, only normalization was applied.
The results obtained are slightly lower than those obtained with the previous technique, which is why only min-max scaling was applied to the final dataset.
\begin{figure}[htb]
    \center
    \includegraphics[width=0.75\linewidth]{figures/data_transformation.png}    
    \caption{Comparision between normalization and minmax scaling}
    \label{fig:plot_data_transformation}
\end{figure}

% TODO: manage position and dimensions of the figures
\subsection*{Data balancing}
The dataset from all previous stages is strongly unbalanced, as shown in Figure \ref{fig:unbalanced}.
\begin{figure}[h]
    \center
    \includegraphics[width=0.6\linewidth]{figures/class_distribution_initial.png}    
    \caption{Initial class distribuition over the samples.}
    \label{fig:unbalanced}
\end{figure}

It is therefore necessary to apply some balancing technique in order to obtain a similar number of samples for each class.
Since the imbalanced-learn \cite{JMLR:v18:16-365} library provides multiple techniques, it was necessary to carry out tests to see which of these gave the best results.
To do so the function mentioned in the previous paragraph was used with the following under/over sampler techniques:
\begin{itemize}
    \item SMOTE
    \item SMOTETomek
    \item SMOTEENN
    \item RandomOverSampler
    \item SMOTE with threshold
\end{itemize}

From the outcomes, some graphs were produced to better interpret the results.
\begin{figure}[h]
    \center
    \includegraphics[width=0.6\linewidth]{figures/imbalance_evaluation.png}
    \caption{Dataset balancing comparison.}
    \label{fig:balance_evaluation}
\end{figure}

As showed in Figure \ref{fig:balance_evaluation} the SMOTE technique gave slightly better results than the other balancing methods.
For this reason it has been applied to the dataset and the dsitribution of samples per class is plotted in Figure \ref{fig:class_distribuition_balance}.
\begin{figure}[h]
    \center
    \includegraphics[width=0.6\linewidth]{figures/class_distribution_smote.png}
    \caption{Class distribution  with SMOTE.}
    \label{fig:class_distribuition_balance}
\end{figure}

Due to the different behaviour of the neural network, it was decided to use one of the methods provided by the PyTorch framework to handle data imbalance:
\begin{itemize}
    \item \textit{WeightedRandomSampler}, gives a weight to each sample considering the frequency of the class it belongs to
\end{itemize}

\section{Modeling}
In order to train the proposed models with enhanced reliability the internal-external cross-validation is used.
In particular, the dataset has been splitted into the train and test sets with the 5-fold cross-validation, then the same technique has been applied, but this time the purpose was to perform hyperparameters optimization.
Moreover, each cross-validator takes the classes distribution into account.
The models specified in the assignments were used with the following hyperparameters:
\begin{itemize}
    \item DecisionTree
    \begin{itemize}
        \item \textit{criterion}, the function to measure the quality of a split: \{gini, entropy\}
        \item \textit{max\_depth}: \{5, 10, 15\}
    \end{itemize}
    \item RandomForest
    \begin{itemize}
        \item \textit{n\_estimator}, the number of trees in the forest: \{700, 900, 1100\}
        \item \textit{max\_features}, the number of features to consider when looking for the best split: \{sqrt, log2\}
    \end{itemize} 
\end{itemize}

\begin{itemize}
    \item GaussianNB
    \begin{itemize}
        \item \textit{var\_smoothing}: logspace(0, -9, num=100)
    \end{itemize}
    \item Quadratic Discriminant Analysis (QDA)
    \begin{itemize}
        \item \textit{reg\_param}: $\{x = 10^z, z \in \mathbb{Z}|-5 \leq z \leq -1\}$
        \item \textit{tol}: $\{x = 10^z, z \in \mathbb{Z}|-4 \leq z \leq -1\}$
    \end{itemize}
\end{itemize}

\begin{itemize}
    \item SVM
    \begin{itemize}
        \item \textit{kernel}: \{rbf, poly, sigmoid\}
        \item \textit{C}: $\{x = 10^z, z \in \mathbb{Z}|-1 \leq z \leq 2\}$
        \item \textit{gamma}: $\{x = 10^z, z \in \mathbb{Z}|-2 \leq z \leq -1\}$
    \end{itemize}
\end{itemize}
\newpage
The previously defined hyperparameters were chosen from the documentation of the various models.
In particular, for categorical parameters, a sub-set of those proposed in the model documentation was chosen, while for numerical parameters, a range was created from the default value.
\begin{itemize}
    \item Neural Network
    \begin{itemize}
        \item \textit{num\_epochs}: \{200\}
        \item \textit{starting\_lr}: $\{10^{-3}\}$
        \item \textit{batch\_size}: \{128, 256\}
        \item \textit{optim}: \{Adam, SGD\}
        \item \textit{momentum}, used only with SGD: \{0.6, 0.9\}
        \item \textit{weight\_decay}: $\{10^{-5}, 10^{-7}\}$
    \end{itemize}
\end{itemize}

With regard to the Neural Network, it was necessary to define also the architecture:
\begin{itemize}
    \item \textit{input\_act}: \{LeakyReLU\}
    \item \textit{hidden\_act}: \{LeakyReLU\}
    \item \textit{hidden\_size}: \{512\}
    \item \textit{num\_hidden\_layers}: \{3, 5\}
    \item \textit{dropout}: \{0.2, 0.3, 0.5\}
    \item \textit{batch\_norm}: \{False, True\}
    \item \textit{loss\_fn}: \{CrossEntropy\}
    \item \textit{output\_fn}: not used because CrossEntropy includes the SoftMax
\end{itemize}

\section{Performance Analysis}
In the case of study, since the dataset is strongly unbalanced, the validation f1-score is the chosen score with which the ranking of models and hyperparameters combinations is performed.
At the end of each training fold on a specific configuration, the measured values are \textit{loss}, \textit{accuracy} and \textit{f1-score}.
Saving these values for each fold, the mean and the confidence interval could be calculated for the chosen metric.

\cleardoublepage

\end{document}